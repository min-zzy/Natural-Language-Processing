{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1**"
      ],
      "metadata": {
        "id": "0Itl9vttF3TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1: A Baseline Neural Netowrk Tagger"
      ],
      "metadata": {
        "id": "EgMvxgWXF7Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xSEAfq7ok0qJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(file):\n",
        "    \"\"\"\n",
        "    Parse the TSV file for the POS tagging task.\n",
        "    Args:\n",
        "    file (file object): The file object to be read.\n",
        "    Returns:\n",
        "    list: A list of sentences, where each sentence is represented as a list of (word, tag) tuples.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            word, pos = line.split('\\t')\n",
        "            sentence.append((word, pos))\n",
        "        else:\n",
        "            sentences.append(sentence)\n",
        "            sentence = []\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "rt_bdefEFk8k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "def load_data(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = parse_file(file)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "2NkZsoEYUcNa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = load_data(\"twpos-train.tsv\")\n",
        "dev_sentences = load_data(\"twpos-dev.tsv\")\n",
        "devtest_sentences = load_data(\"twpos-devtest.tsv\")"
      ],
      "metadata": {
        "id": "w5Xv1RAU20Wm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    train_data = train_sentences\n",
        "\n",
        "    word_to_ix = {\"UUUNKKK\": 0, '<s>':1, '</s>':2}\n",
        "    tag_to_ix = {}\n",
        "\n",
        "    # Iterate over each sentence (which is a list of (word, tag) tuples)\n",
        "    for sentence in train_data:\n",
        "        for word, tag in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                # Add the word to the dictionary if it's not already there\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "            if tag not in tag_to_ix:\n",
        "                tag_to_ix[tag] = len(tag_to_ix)\n",
        "    return word_to_ix, tag_to_ix\n",
        "\n",
        "# Usage\n",
        "word_to_ix, tag_to_ix = prepare_data()\n",
        "target_size = len(tag_to_ix)"
      ],
      "metadata": {
        "id": "I1R0iiSZ3DSw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_sentence(data, word_to_ix, tag_to_ix, w):\n",
        "  concat_data = []\n",
        "  tags = []\n",
        "  for sentence in data:\n",
        "      for i in range(len(sentence)):\n",
        "          current = []\n",
        "          for j in range(i - w, i + w + 1):\n",
        "              if j < 0:\n",
        "                word = '</s>'\n",
        "              elif j >= len(sentence):\n",
        "                word = '</s>'\n",
        "              else:\n",
        "                word = sentence[j][0]\n",
        "              if word not in word_to_ix:\n",
        "                word = 'UUUNKKK'\n",
        "              current.append(word_to_ix[word])\n",
        "              if i == j:\n",
        "                tag = tag_to_ix[sentence[j][1]]\n",
        "          concat_data.append(current)\n",
        "          tags.append(tag)\n",
        "  return np.array(concat_data), np.array(tags)"
      ],
      "metadata": {
        "id": "vZvdT2l1Pwfn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger:\n",
        "  def __init__(self, w, vocab_size):\n",
        "    self.w = w\n",
        "    self.dim_e = 50\n",
        "    self.dim_h = 128\n",
        "    self.dim_s = len(tag_to_ix)\n",
        "    self.vocab_size = len(word_to_ix)\n",
        "    self.batch_size = 32\n",
        "    self.epochs = 20"
      ],
      "metadata": {
        "id": "IUMg6Ff0t_vv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, POSTagger):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        #print(embeds.shape)\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, word_to_ix, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "EppCeeOr2o0f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment demonstrated a clear performance improvement when the context window size was increased from w=0 to w=1, highlighting the model's enhanced ability to leverage additional contextual information for more accurate predictions. This suggests that a larger context window is crucial for capturing necessary context and nuances in language processing tasks."
      ],
      "metadata": {
        "id": "UUnx2FnaqFi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger(w=w, vocab_size = len(word_to_ix))\n",
        "        model = NN(POST)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, word_to_ix, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjhluO6bgYfD",
        "outputId": "0597261e-ea2d-42a7-f0f5-43de22140247"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0, Loss: 2.5841\n",
            "Loss: 1.9774, Accuracy: 35.4491%\n",
            "Epoch 1, Loss: 1.5853\n",
            "Loss: 1.3387, Accuracy: 66.4592%\n",
            "Epoch 2, Loss: 1.0291\n",
            "Loss: 1.0984, Accuracy: 74.0095%\n",
            "Epoch 3, Loss: 0.7097\n",
            "Loss: 1.1080, Accuracy: 75.2541%\n",
            "Epoch 4, Loss: 0.5257\n",
            "Loss: 1.1066, Accuracy: 76.0216%\n",
            "Epoch 5, Loss: 0.4369\n",
            "Loss: 1.1484, Accuracy: 76.9965%\n",
            "Epoch 6, Loss: 0.3771\n",
            "Loss: 1.1062, Accuracy: 77.2869%\n",
            "Epoch 7, Loss: 0.3444\n",
            "Loss: 1.0896, Accuracy: 77.0794%\n",
            "Epoch 8, Loss: 0.3209\n",
            "Loss: 1.2372, Accuracy: 76.9342%\n",
            "Epoch 9, Loss: 0.2997\n",
            "Loss: 1.2117, Accuracy: 77.1624%\n",
            "Epoch 10, Loss: 0.2864\n",
            "Loss: 1.2560, Accuracy: 76.1253%\n",
            "Epoch 11, Loss: 0.2770\n",
            "Loss: 1.2357, Accuracy: 77.4321%\n",
            "Epoch 12, Loss: 0.2673\n",
            "Loss: 1.1994, Accuracy: 77.4528%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 1.0739, Accuracy: 79.4352%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0, Loss: 2.4746\n",
            "Loss: 1.4940, Accuracy: 55.4242%\n",
            "Epoch 1, Loss: 1.0119\n",
            "Loss: 0.9146, Accuracy: 73.7191%\n",
            "Epoch 2, Loss: 0.6426\n",
            "Loss: 0.8944, Accuracy: 77.7432%\n",
            "Epoch 3, Loss: 0.4733\n",
            "Loss: 0.9049, Accuracy: 79.4648%\n",
            "Epoch 4, Loss: 0.3708\n",
            "Loss: 0.9125, Accuracy: 80.0249%\n",
            "Epoch 5, Loss: 0.2963\n",
            "Loss: 0.8849, Accuracy: 80.8961%\n",
            "Epoch 6, Loss: 0.2306\n",
            "Loss: 0.9429, Accuracy: 80.0041%\n",
            "Epoch 7, Loss: 0.1694\n",
            "Loss: 1.0442, Accuracy: 78.4485%\n",
            "Epoch 8, Loss: 0.1331\n",
            "Loss: 1.0923, Accuracy: 79.5478%\n",
            "Epoch 9, Loss: 0.1039\n",
            "Loss: 1.1291, Accuracy: 79.2367%\n",
            "Epoch 10, Loss: 0.0859\n",
            "Loss: 1.1201, Accuracy: 80.1079%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.9704, Accuracy: 80.9873%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Feature Engineering"
      ],
      "metadata": {
        "id": "Mlmv_c2xGKm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon reviewing misclassified center words from section 1.1, it was observed that inaccuracies often involved words beginning with capital letters, containing special characters, or including numbers. To address this, new features were designed to precisely identify such traits within the words. Incorporating these nuanced features enhanced the model's understanding of the data, leading to a noticeable improvement in accuracy compared to the results in section 1.1."
      ],
      "metadata": {
        "id": "kPTlJmn7rLq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def extract_features(word):\n",
        "    features = [\n",
        "        float(any(char.isdigit() for char in word)),  # contains digits\n",
        "        float(word.isdigit()),\n",
        "        float(any(char in string.punctuation for char in word)),  # contains punctuation\n",
        "        word[0] in string.punctuation,\n",
        "        len(word)\n",
        "    ]\n",
        "    return features"
      ],
      "metadata": {
        "id": "jGZoxxyh6ME0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_sentence_features(sentences, w):\n",
        "  feature_matrix = []\n",
        "  for sentence in sentences:\n",
        "      for i in range(len(sentence)):\n",
        "          for j in range(i - w, i + w + 1):\n",
        "                if i == j:\n",
        "                  features = extract_features(sentence[j][0])\n",
        "                  feature_matrix.append(features)\n",
        "  return np.array(feature_matrix)"
      ],
      "metadata": {
        "id": "9UZQmk73064c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_feature(nn.Module):\n",
        "    def __init__(self, POSTagger):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.num_features = 5\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e  + self.num_features, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs, features = inputs\n",
        "        embeds = self.embeddings(inputs).view(-1, ((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e))\n",
        "        combined = torch.cat((embeds, features), 1)\n",
        "        hidden_out = self.hidden(combined)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, word_to_ix, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.train_features = transfer_sentence_features(train_sentence, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_features = transfer_sentence_features(dev_sentence, self.POSTagger.w)\n",
        "\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_features = transfer_sentence_features(devtest_sentence, self.POSTagger.w)\n",
        "\n",
        "        # Convert the feature data from NumPy arrays to tensors\n",
        "        self.train_features = torch.tensor(self.train_features, dtype=torch.float)\n",
        "        self.dev_features = torch.tensor(self.dev_features, dtype=torch.float)\n",
        "        self.devtest_features = torch.tensor(self.devtest_features, dtype=torch.float)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label, features):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "        feature_s = features[idx]\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            feature_batch = feature_s[i:i + self.POSTagger.batch_size].clone().detach()\n",
        "\n",
        "            loss = self.run_grad((x, feature_batch), y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label, features):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            feature_batch = features.clone().detach()\n",
        "            loss = self.get_loss((x, feature_batch), y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward((x,feature_batch))\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "        test_loss /= n\n",
        "        accuracy = (correct / n) * 100\n",
        "        print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "        return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label, self.train_features)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label, self.dev_features)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label, self.devtest_features)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "2bl71q6nhHSv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger(w=w, vocab_size = len(word_to_ix))\n",
        "        model = NN_feature(POST)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, word_to_ix, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0fDjYdvppi8",
        "outputId": "d597604e-4dcd-405f-c785-c9ffb614937e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0, Loss: 1.9440\n",
            "Loss: 1.4169, Accuracy: 51.2342%\n",
            "Epoch 1, Loss: 1.2304\n",
            "Loss: 1.0155, Accuracy: 66.8741%\n",
            "Epoch 2, Loss: 0.9468\n",
            "Loss: 0.8914, Accuracy: 74.5281%\n",
            "Epoch 3, Loss: 0.7549\n",
            "Loss: 0.9036, Accuracy: 72.6613%\n",
            "Epoch 4, Loss: 0.6064\n",
            "Loss: 0.9844, Accuracy: 76.6646%\n",
            "Epoch 5, Loss: 0.5284\n",
            "Loss: 0.9027, Accuracy: 77.9091%\n",
            "Epoch 6, Loss: 0.4545\n",
            "Loss: 1.0288, Accuracy: 75.4615%\n",
            "Epoch 7, Loss: 0.3969\n",
            "Loss: 0.8957, Accuracy: 79.1122%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.8610, Accuracy: 79.3706%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0, Loss: 1.9276\n",
            "Loss: 1.2637, Accuracy: 60.4854%\n",
            "Epoch 1, Loss: 1.1189\n",
            "Loss: 1.0785, Accuracy: 67.2682%\n",
            "Epoch 2, Loss: 0.8164\n",
            "Loss: 1.1181, Accuracy: 69.8818%\n",
            "Epoch 3, Loss: 0.6465\n",
            "Loss: 0.7750, Accuracy: 78.5522%\n",
            "Epoch 4, Loss: 0.5083\n",
            "Loss: 0.7405, Accuracy: 80.5227%\n",
            "Epoch 5, Loss: 0.4136\n",
            "Loss: 0.7404, Accuracy: 80.6472%\n",
            "Epoch 6, Loss: 0.3367\n",
            "Loss: 0.7324, Accuracy: 80.7716%\n",
            "Epoch 7, Loss: 0.2851\n",
            "Loss: 0.8485, Accuracy: 80.5642%\n",
            "Epoch 8, Loss: 0.2369\n",
            "Loss: 0.8091, Accuracy: 80.8753%\n",
            "Epoch 9, Loss: 0.2029\n",
            "Loss: 0.8677, Accuracy: 81.6843%\n",
            "Epoch 10, Loss: 0.1719\n",
            "Loss: 0.8518, Accuracy: 82.1406%\n",
            "Epoch 11, Loss: 0.1415\n",
            "Loss: 0.8724, Accuracy: 82.1199%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.8130, Accuracy: 83.2291%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Pretrained Embeddings"
      ],
      "metadata": {
        "id": "BdD3oXi4uCLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Experiment with updating the pretrained embeddings for w = 0 and w = 1"
      ],
      "metadata": {
        "id": "krSXD6h8UUsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Setting up the train an embedding for '&lt;s&gt;' by using the embedding for '&lt;s&gt;'"
      ],
      "metadata": {
        "id": "gggKl_PQsFaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def load_embeddings(embedding_file):\n",
        "    embeddings = {}\n",
        "    with open(embedding_file, 'r', encoding='utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        words = line.split()\n",
        "        embeddings[words[0]] = words[1:]\n",
        "        if words[0] == '<\\s>':\n",
        "          embeddings['<s>'] = words[1:]\n",
        "    return pd.DataFrame.from_dict(embeddings, orient = 'index').astype('float32')"
      ],
      "metadata": {
        "id": "jqJwin-oFOkh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_df = load_embeddings('/content/twitter-embeddings.txt')"
      ],
      "metadata": {
        "id": "BAEjTgjRJIz_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_to_index = {word: i for i, word in enumerate(embedding_df.index)}"
      ],
      "metadata": {
        "id": "M-yIPcuEL0T0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger_embed:\n",
        "  def __init__(self, w, vocab_size):\n",
        "    self.w = w\n",
        "    self.dim_e = 50\n",
        "    self.dim_h = 128\n",
        "    self.dim_s = len(tag_to_ix)\n",
        "    self.vocab_size = len(twitter_to_index)\n",
        "    self.batch_size = 32\n",
        "    self.epochs = 20"
      ],
      "metadata": {
        "id": "KOAMqHxSP44f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_embed(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        #print(embeds.shape)\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "N3XALycWE8fn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment demonstrated accuracy improvements for both w=0 and w=1 settings when using pre-trained embeddings, as opposed to randomly-initialized word embeddings. This underscores the superiority of adopting pre-trained embeddings for enhanced model performance."
      ],
      "metadata": {
        "id": "lFHuBDyqtnh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger_embed(w=w, vocab_size = len(embedding_df))\n",
        "        model = NN_embed(POST, pretrained_embeddings, twitter_to_index)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq7LT2znGxXF",
        "outputId": "05a0b633-b624-4e6a-8ace-8ffd35f859cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0297\n",
            "Loss: 0.6024, Accuracy: 83.0741%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5149\n",
            "Loss: 0.5584, Accuracy: 83.2607%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4381\n",
            "Loss: 0.5671, Accuracy: 82.9081%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4112\n",
            "Loss: 0.5521, Accuracy: 83.9038%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3913\n",
            "Loss: 0.5605, Accuracy: 82.8666%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3755\n",
            "Loss: 0.5554, Accuracy: 82.0369%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3674\n",
            "Loss: 0.5815, Accuracy: 82.9911%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3611\n",
            "Loss: 0.5542, Accuracy: 84.0075%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3547\n",
            "Loss: 0.5628, Accuracy: 83.3230%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5226, Accuracy: 83.9405%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.5896\n",
            "Loss: 0.5271, Accuracy: 86.0610%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.2796\n",
            "Loss: 0.6146, Accuracy: 85.0239%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2363\n",
            "Loss: 0.5103, Accuracy: 86.4966%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2054\n",
            "Loss: 0.5199, Accuracy: 86.9322%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1816\n",
            "Loss: 0.5347, Accuracy: 87.0359%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1517\n",
            "Loss: 0.6381, Accuracy: 85.8121%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1350\n",
            "Loss: 0.6139, Accuracy: 86.2477%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1178\n",
            "Loss: 0.6455, Accuracy: 86.4758%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5906, Accuracy: 86.9153%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Comparing the result for updating the pretrained word embeddings during training and keeping them fixed"
      ],
      "metadata": {
        "id": "1Z2Zg3VCUWnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_embed_freeze(nn.Module):\n",
        "    def __init__(self, POSTagger, pretrained_embeddings, twitter_to_index, freeze_embeddings):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.freeze_embeddings = freeze_embeddings\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.parameters()), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.embeddings.weight.requires_grad = not self.freeze_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "\n",
        "            print('------------------------------------------')\n",
        "\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "HcjNJmMCUbw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training with a context window of w=1, updating pre-trained word embeddings has shown distinct benefits, evidenced by a modest yet noteworthy improvement of approximately 2% in accuracy compared to keeping them fixed. This enhancement underscores the effectiveness of allowing the embeddings to evolve with training."
      ],
      "metadata": {
        "id": "lmnhTRiYvrO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    freeze = [True, False]\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    for f in freeze:\n",
        "        print(f\"\\nParameter Freeze = {f}\")\n",
        "        POST = POSTagger_embed(w = 1, vocab_size = len(embedding_df))\n",
        "        model = NN_embed_freeze(POST, pretrained_embeddings, twitter_to_index, f)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "id": "VViBnjdYlG51",
        "outputId": "fe95fb49-93e0-4de4-cbff-f50e09968236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameter Freeze = True\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0185\n",
            "Loss: 0.6198, Accuracy: 82.5762%\n",
            "------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5551\n",
            "Loss: 0.5629, Accuracy: 84.4638%\n",
            "------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5009\n",
            "Loss: 0.5363, Accuracy: 84.6712%\n",
            "------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4689\n",
            "Loss: 0.5310, Accuracy: 84.6505%\n",
            "------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4453\n",
            "Loss: 0.4982, Accuracy: 85.3350%\n",
            "------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.4238\n",
            "Loss: 0.5020, Accuracy: 86.2892%\n",
            "------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.4062\n",
            "Loss: 0.5099, Accuracy: 84.8164%\n",
            "------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3886\n",
            "Loss: 0.4973, Accuracy: 85.7084%\n",
            "------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3689\n",
            "Loss: 0.4916, Accuracy: 86.2684%\n",
            "------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3515\n",
            "Loss: 0.5043, Accuracy: 86.7040%\n",
            "------------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3394\n",
            "Loss: 0.5160, Accuracy: 85.5632%\n",
            "------------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3252\n",
            "Loss: 0.5030, Accuracy: 86.2269%\n",
            "------------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.3058\n",
            "Loss: 0.5014, Accuracy: 86.6833%\n",
            "------------------------------------------\n",
            "Epoch 13\n",
            "Epoch 13, Loss: 0.2974\n",
            "Loss: 0.5296, Accuracy: 85.7498%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5189, Accuracy: 85.4710%\n",
            "\n",
            "Parameter Freeze = False\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9244\n",
            "Loss: 0.5209, Accuracy: 85.3765%\n",
            "------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4087\n",
            "Loss: 0.4828, Accuracy: 86.2062%\n",
            "------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3050\n",
            "Loss: 0.4849, Accuracy: 86.7662%\n",
            "------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2462\n",
            "Loss: 0.5116, Accuracy: 86.4966%\n",
            "------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2033\n",
            "Loss: 0.5295, Accuracy: 86.8699%\n",
            "------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1736\n",
            "Loss: 0.5659, Accuracy: 86.1440%\n",
            "------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1481\n",
            "Loss: 0.5922, Accuracy: 86.4136%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5216, Accuracy: 87.1955%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Feature Engineering for pretrained embeddings model"
      ],
      "metadata": {
        "id": "pnvbzmOoYRmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_embed_feature(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.num_features = 5\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e  + self.num_features, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "    def forward(self, inputs):\n",
        "        inputs, features = inputs\n",
        "        embeds = self.embeddings(inputs).view(-1, ((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e))\n",
        "        combined = torch.cat((embeds, features), 1)\n",
        "        hidden_out = self.hidden(combined)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, word_to_ix, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.train_features = transfer_sentence_features(train_sentence, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_features = transfer_sentence_features(dev_sentence, self.POSTagger.w)\n",
        "\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_features = transfer_sentence_features(devtest_sentence, self.POSTagger.w)\n",
        "\n",
        "        # Convert the feature data from NumPy arrays to tensors\n",
        "        self.train_features = torch.tensor(self.train_features, dtype=torch.float)\n",
        "        self.dev_features = torch.tensor(self.dev_features, dtype=torch.float)\n",
        "        self.devtest_features = torch.tensor(self.devtest_features, dtype=torch.float)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label, features):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "        feature_s = features[idx]\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            feature_batch = feature_s[i:i + self.POSTagger.batch_size].clone().detach()\n",
        "            loss = self.run_grad((x, feature_batch), y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label, features):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            feature_batch = features.clone().detach()\n",
        "            loss = self.get_loss((x, feature_batch), y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward((x,feature_batch))\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "        test_loss /= n\n",
        "        accuracy = (correct / n) * 100\n",
        "        print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "        return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label, self.train_features)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label, self.dev_features)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label, self.devtest_features)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "c4Iio5OiYcGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining the custom features from Section 1.2 with pre-trained embeddings reveals that these features still offer assistance. Although the improvement is subtle, it highlights that features can contribute additional context that pretraining might not capture, thereby providing a slight edge in the model's performance."
      ],
      "metadata": {
        "id": "7ua-serjxtKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger_embed(w=w, vocab_size = len(embedding_df))\n",
        "        model = NN_embed_feature(POST, pretrained_embeddings, twitter_to_index)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "id": "qYKBx8DoznpA",
        "outputId": "31bec63b-fbcf-450c-a0f9-820e8c498398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.2516\n",
            "Loss: 0.6053, Accuracy: 83.0533%\n",
            "------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.6132\n",
            "Loss: 0.6372, Accuracy: 82.0577%\n",
            "------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5005\n",
            "Loss: 0.5374, Accuracy: 85.2520%\n",
            "------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4427\n",
            "Loss: 0.6016, Accuracy: 83.0533%\n",
            "------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3948\n",
            "Loss: 0.5673, Accuracy: 84.0904%\n",
            "------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3778\n",
            "Loss: 0.5583, Accuracy: 83.7586%\n",
            "------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3566\n",
            "Loss: 0.6181, Accuracy: 83.5096%\n",
            "------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3525\n",
            "Loss: 0.5672, Accuracy: 84.3808%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5316, Accuracy: 84.4794%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.7329\n",
            "Loss: 0.5072, Accuracy: 86.6833%\n",
            "------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3052\n",
            "Loss: 0.5249, Accuracy: 86.1854%\n",
            "------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2605\n",
            "Loss: 0.4829, Accuracy: 87.2641%\n",
            "------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2237\n",
            "Loss: 0.6151, Accuracy: 85.8328%\n",
            "------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1973\n",
            "Loss: 0.5717, Accuracy: 85.6254%\n",
            "------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1795\n",
            "Loss: 0.5281, Accuracy: 87.5959%\n",
            "------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1579\n",
            "Loss: 0.5908, Accuracy: 86.5381%\n",
            "------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1472\n",
            "Loss: 0.6110, Accuracy: 86.7870%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5496, Accuracy: 87.5620%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Architecture Engineering"
      ],
      "metadata": {
        "id": "XdfzMAZgnCs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I got the best test accuracy when window size = 1 and no hidden layer with tanh function.**"
      ],
      "metadata": {
        "id": "itGBu_pfCCe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Compare the use of 0, 1, and 2 hidden layers"
      ],
      "metadata": {
        "id": "Q99h1r2mnGGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_layers(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index, hidden_dim1=None, hidden_dim2=None):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "        self.hidden1 = None\n",
        "        self.hidden2 = None\n",
        "\n",
        "        input_dim = (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e\n",
        "\n",
        "        if hidden_dim1:\n",
        "            self.hidden1 = nn.Linear(input_dim, hidden_dim1)\n",
        "            output_dim = hidden_dim1\n",
        "\n",
        "            if hidden_dim2:\n",
        "                self.hidden2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "                output_dim = hidden_dim2\n",
        "        else:\n",
        "            output_dim = input_dim\n",
        "        self.output = nn.Linear(output_dim, self.POSTagger.dim_s)\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        if self.hidden1 is not None:\n",
        "          self.hidden1.weight.data.uniform_(-initrange, initrange)\n",
        "        if self.hidden2 is not None:\n",
        "          self.hidden2.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        if self.hidden1 is not None:\n",
        "          hidden_out = self.hidden1(embeds)\n",
        "          if self.hidden2 is not None:\n",
        "            hidden_out = self.hidden2(hidden_out)\n",
        "        else:\n",
        "          hidden_out = embeds\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('---------------------------------------------')\n",
        "        print('\\n Testing Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "E4NsmWgFnEyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [\n",
        "    # 0 hidden layers\n",
        "   {'hidden_layers': 0, 'hidden_dim1': None, 'hidden_dim2': None},\n",
        "\n",
        "    # 1 hidden layer, two setups\n",
        "    {'hidden_layers': 1, 'hidden_dim1': 256, 'hidden_dim2': None},\n",
        "    {'hidden_layers': 1, 'hidden_dim1': 512, 'hidden_dim2': None},\n",
        "\n",
        "    # 2 hidden layers, two setups\n",
        "    {'hidden_layers': 2, 'hidden_dim1': 256, 'hidden_dim2': 128},\n",
        "    {'hidden_layers': 2, 'hidden_dim1': 512, 'hidden_dim2': 256}\n",
        "]\n",
        "\n",
        "# Adding window sizes to test\n",
        "window_sizes = [0, 1]\n",
        "\n",
        "# This will be used to collect all results\n",
        "all_results = []\n",
        "\n",
        "for window in window_sizes:\n",
        "    results = []  # This collects results for the current window size\n",
        "\n",
        "    for config in configs:\n",
        "        print(f\"Training configuration: {config}, Window size: {window}\")\n",
        "\n",
        "        # Initialize the model with the current configuration\n",
        "        pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "        POST = POSTagger_embed(w=window, vocab_size=len(embedding_df))\n",
        "        model = NN_layers(POST, pretrained_embeddings, twitter_to_index,\n",
        "                          hidden_dim1=config['hidden_dim1'],\n",
        "                          hidden_dim2=config['hidden_dim2'])\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "\n",
        "        result = model.fit()\n",
        "\n",
        "        if isinstance(result, tuple):\n",
        "            result = {\n",
        "                'train_loss': result[0],\n",
        "                'dev_loss': result[1],\n",
        "                'devtest_loss': result[2],\n",
        "                'dev_accuracy': result[3],\n",
        "                'devtest_accuracy': result[4]\n",
        "            }\n",
        "\n",
        "        result['window_size'] = window\n",
        "        result.update(config)\n",
        "        results.append(result)\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(f\"Results for window size {window}:\\n\", results_df)\n",
        "    all_results.extend(results)\n",
        "all_results_df = pd.DataFrame(all_results)\n",
        "print(\"Complete results:\\n\", all_results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPHzz6n8xkfH",
        "outputId": "c9845acc-b5ab-477a-ba4a-0053d31ec89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration: {'hidden_layers': 0, 'hidden_dim1': None, 'hidden_dim2': None}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9956\n",
            "Loss: 0.6288, Accuracy: 82.4933%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5289\n",
            "Loss: 0.5461, Accuracy: 83.7793%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4469\n",
            "Loss: 0.5349, Accuracy: 83.9038%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4000\n",
            "Loss: 0.5340, Accuracy: 84.4638%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3757\n",
            "Loss: 0.5239, Accuracy: 84.0697%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3556\n",
            "Loss: 0.5308, Accuracy: 83.8415%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3457\n",
            "Loss: 0.5314, Accuracy: 83.9660%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3396\n",
            "Loss: 0.5453, Accuracy: 84.2771%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3326\n",
            "Loss: 0.5715, Accuracy: 83.8623%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3306\n",
            "Loss: 0.5480, Accuracy: 83.4889%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5004, Accuracy: 83.8112%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 256, 'hidden_dim2': None}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9934\n",
            "Loss: 0.6038, Accuracy: 83.4474%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5090\n",
            "Loss: 0.5578, Accuracy: 84.4431%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4407\n",
            "Loss: 0.5778, Accuracy: 83.6341%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4079\n",
            "Loss: 0.5777, Accuracy: 83.2607%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3904\n",
            "Loss: 0.5496, Accuracy: 83.9245%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3764\n",
            "Loss: 0.5602, Accuracy: 83.1985%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3687\n",
            "Loss: 0.5546, Accuracy: 82.7214%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3638\n",
            "Loss: 0.5599, Accuracy: 82.8874%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3585\n",
            "Loss: 0.5555, Accuracy: 83.9038%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3558\n",
            "Loss: 0.5725, Accuracy: 83.3230%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5270, Accuracy: 83.7465%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 512, 'hidden_dim2': None}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9611\n",
            "Loss: 0.6097, Accuracy: 82.9081%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5073\n",
            "Loss: 0.5798, Accuracy: 83.3022%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4405\n",
            "Loss: 0.5681, Accuracy: 83.7171%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4047\n",
            "Loss: 0.5881, Accuracy: 83.2192%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3898\n",
            "Loss: 0.5718, Accuracy: 83.9867%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3784\n",
            "Loss: 0.5846, Accuracy: 82.2651%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3693\n",
            "Loss: 0.5529, Accuracy: 83.9660%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3652\n",
            "Loss: 0.5481, Accuracy: 84.3393%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3585\n",
            "Loss: 0.5684, Accuracy: 82.7007%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3539\n",
            "Loss: 0.6038, Accuracy: 82.0784%\n",
            "---------------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3507\n",
            "Loss: 0.5604, Accuracy: 83.6963%\n",
            "---------------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3475\n",
            "Loss: 0.6018, Accuracy: 83.1155%\n",
            "---------------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.3413\n",
            "Loss: 0.5608, Accuracy: 83.2815%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5116, Accuracy: 83.6818%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 256, 'hidden_dim2': 128}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.5243\n",
            "Loss: 0.8416, Accuracy: 78.7803%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.7018\n",
            "Loss: 0.6756, Accuracy: 82.0369%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5337\n",
            "Loss: 0.6534, Accuracy: 81.3317%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4484\n",
            "Loss: 0.6439, Accuracy: 81.7465%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4165\n",
            "Loss: 0.6497, Accuracy: 81.4561%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3864\n",
            "Loss: 0.6501, Accuracy: 81.9747%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3740\n",
            "Loss: 0.6322, Accuracy: 83.0326%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3599\n",
            "Loss: 0.6353, Accuracy: 82.5555%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3492\n",
            "Loss: 0.6759, Accuracy: 82.0784%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3446\n",
            "Loss: 0.6422, Accuracy: 82.9081%\n",
            "---------------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3404\n",
            "Loss: 0.6706, Accuracy: 81.6221%\n",
            "---------------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3339\n",
            "Loss: 0.6811, Accuracy: 81.9540%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.6297, Accuracy: 82.8842%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 512, 'hidden_dim2': 256}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.4387\n",
            "Loss: 0.7992, Accuracy: 78.7389%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.6634\n",
            "Loss: 0.6757, Accuracy: 81.7258%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5206\n",
            "Loss: 0.6229, Accuracy: 81.9747%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4598\n",
            "Loss: 0.6050, Accuracy: 82.8874%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4322\n",
            "Loss: 0.6386, Accuracy: 83.5096%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.4159\n",
            "Loss: 0.6184, Accuracy: 81.7880%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3959\n",
            "Loss: 0.6161, Accuracy: 81.8088%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3852\n",
            "Loss: 0.6160, Accuracy: 82.2236%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3810\n",
            "Loss: 0.6153, Accuracy: 81.3524%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5506, Accuracy: 82.7980%\n",
            "Results for window size 0:\n",
            "    train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.330638  0.547994      0.500398     83.488903         83.811166   \n",
            "1    0.355798  0.572516      0.526975     83.322962         83.746497   \n",
            "2    0.341285  0.560836      0.511553     83.281477         83.681828   \n",
            "3    0.333919  0.681067      0.629749     81.953951         82.884242   \n",
            "4    0.381044  0.615315      0.550612     81.352417         82.798017   \n",
            "\n",
            "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
            "0            0              0          NaN          NaN  \n",
            "1            0              1        256.0          NaN  \n",
            "2            0              1        512.0          NaN  \n",
            "3            0              2        256.0        128.0  \n",
            "4            0              2        512.0        256.0  \n",
            "Training configuration: {'hidden_layers': 0, 'hidden_dim1': None, 'hidden_dim2': None}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.8744\n",
            "Loss: 0.5314, Accuracy: 85.9573%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4235\n",
            "Loss: 0.4615, Accuracy: 87.1603%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3303\n",
            "Loss: 0.4287, Accuracy: 87.6582%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2718\n",
            "Loss: 0.4307, Accuracy: 87.6996%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2326\n",
            "Loss: 0.4306, Accuracy: 87.9071%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2034\n",
            "Loss: 0.4433, Accuracy: 87.7204%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1814\n",
            "Loss: 0.4437, Accuracy: 87.9071%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1634\n",
            "Loss: 0.4506, Accuracy: 87.6996%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.4088, Accuracy: 88.4458%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 256, 'hidden_dim2': None}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.8796\n",
            "Loss: 0.5149, Accuracy: 85.2935%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3966\n",
            "Loss: 0.4706, Accuracy: 86.3514%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3039\n",
            "Loss: 0.4762, Accuracy: 86.3306%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2458\n",
            "Loss: 0.5343, Accuracy: 86.3929%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2062\n",
            "Loss: 0.5258, Accuracy: 86.9529%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1730\n",
            "Loss: 0.6037, Accuracy: 85.8536%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1468\n",
            "Loss: 0.5851, Accuracy: 86.4551%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5374, Accuracy: 87.0015%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 512, 'hidden_dim2': None}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.8517\n",
            "Loss: 0.4990, Accuracy: 85.7084%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3961\n",
            "Loss: 0.4837, Accuracy: 85.9988%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2943\n",
            "Loss: 0.5098, Accuracy: 86.6833%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2433\n",
            "Loss: 0.5001, Accuracy: 87.1603%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2057\n",
            "Loss: 0.5438, Accuracy: 86.8077%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1697\n",
            "Loss: 0.5634, Accuracy: 86.7040%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1486\n",
            "Loss: 0.6146, Accuracy: 86.2062%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5514, Accuracy: 86.8937%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 256, 'hidden_dim2': 128}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.4531\n",
            "Loss: 0.7780, Accuracy: 77.9921%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.6033\n",
            "Loss: 0.5929, Accuracy: 84.4223%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4245\n",
            "Loss: 0.5321, Accuracy: 85.2105%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3374\n",
            "Loss: 0.6120, Accuracy: 83.9660%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2864\n",
            "Loss: 0.5939, Accuracy: 84.8579%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2496\n",
            "Loss: 0.6795, Accuracy: 84.0697%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.2119\n",
            "Loss: 0.6627, Accuracy: 84.9616%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1967\n",
            "Loss: 0.6871, Accuracy: 84.5468%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.6488, Accuracy: 85.7081%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 512, 'hidden_dim2': 256}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.3185\n",
            "Loss: 0.6829, Accuracy: 80.5849%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5398\n",
            "Loss: 0.7175, Accuracy: 82.4103%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4067\n",
            "Loss: 0.6373, Accuracy: 82.6799%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3341\n",
            "Loss: 0.5720, Accuracy: 85.7291%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2877\n",
            "Loss: 0.5991, Accuracy: 85.2935%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2462\n",
            "Loss: 0.6604, Accuracy: 84.8994%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.2193\n",
            "Loss: 0.6413, Accuracy: 85.4802%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1879\n",
            "Loss: 0.6814, Accuracy: 84.8994%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.1794\n",
            "Loss: 0.6874, Accuracy: 84.4223%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.6609, Accuracy: 85.4710%\n",
            "Results for window size 1:\n",
            "    train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.163390  0.450630      0.408758     87.699647         88.445786   \n",
            "1    0.146835  0.585115      0.537446     86.455092         87.001509   \n",
            "2    0.148623  0.614594      0.551444     86.206181         86.893727   \n",
            "3    0.196667  0.687148      0.648792     84.546775         85.708127   \n",
            "4    0.179426  0.687404      0.660889     84.422319         85.471007   \n",
            "\n",
            "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
            "0            1              0          NaN          NaN  \n",
            "1            1              1        256.0          NaN  \n",
            "2            1              1        512.0          NaN  \n",
            "3            1              2        256.0        128.0  \n",
            "4            1              2        512.0        256.0  \n",
            "Complete results:\n",
            "    train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.330638  0.547994      0.500398     83.488903         83.811166   \n",
            "1    0.355798  0.572516      0.526975     83.322962         83.746497   \n",
            "2    0.341285  0.560836      0.511553     83.281477         83.681828   \n",
            "3    0.333919  0.681067      0.629749     81.953951         82.884242   \n",
            "4    0.381044  0.615315      0.550612     81.352417         82.798017   \n",
            "5    0.163390  0.450630      0.408758     87.699647         88.445786   \n",
            "6    0.146835  0.585115      0.537446     86.455092         87.001509   \n",
            "7    0.148623  0.614594      0.551444     86.206181         86.893727   \n",
            "8    0.196667  0.687148      0.648792     84.546775         85.708127   \n",
            "9    0.179426  0.687404      0.660889     84.422319         85.471007   \n",
            "\n",
            "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
            "0            0              0          NaN          NaN  \n",
            "1            0              1        256.0          NaN  \n",
            "2            0              1        512.0          NaN  \n",
            "3            0              2        256.0        128.0  \n",
            "4            0              2        512.0        256.0  \n",
            "5            1              0          NaN          NaN  \n",
            "6            1              1        256.0          NaN  \n",
            "7            1              1        512.0          NaN  \n",
            "8            1              2        256.0        128.0  \n",
            "9            1              2        512.0        256.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both for the window sizes 0 and 1, the test accuracy decreased as the model increased the number of layers and neurons in the layers."
      ],
      "metadata": {
        "id": "Mm3fW9bQCu1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results_df"
      ],
      "metadata": {
        "id": "0Lzbn_HLCbiC",
        "outputId": "cf486d41-8516-414c-8be8-844a48195efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.330638  0.547994      0.500398     83.488903         83.811166   \n",
              "1    0.355798  0.572516      0.526975     83.322962         83.746497   \n",
              "2    0.341285  0.560836      0.511553     83.281477         83.681828   \n",
              "3    0.333919  0.681067      0.629749     81.953951         82.884242   \n",
              "4    0.381044  0.615315      0.550612     81.352417         82.798017   \n",
              "5    0.163390  0.450630      0.408758     87.699647         88.445786   \n",
              "6    0.146835  0.585115      0.537446     86.455092         87.001509   \n",
              "7    0.148623  0.614594      0.551444     86.206181         86.893727   \n",
              "8    0.196667  0.687148      0.648792     84.546775         85.708127   \n",
              "9    0.179426  0.687404      0.660889     84.422319         85.471007   \n",
              "\n",
              "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
              "0            0              0          NaN          NaN  \n",
              "1            0              1        256.0          NaN  \n",
              "2            0              1        512.0          NaN  \n",
              "3            0              2        256.0        128.0  \n",
              "4            0              2        512.0        256.0  \n",
              "5            1              0          NaN          NaN  \n",
              "6            1              1        256.0          NaN  \n",
              "7            1              1        512.0          NaN  \n",
              "8            1              2        256.0        128.0  \n",
              "9            1              2        512.0        256.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d0ba17c-8e7d-4aa0-b182-fc4b12b96636\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>hidden_dim1</th>\n",
              "      <th>hidden_dim2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.330638</td>\n",
              "      <td>0.547994</td>\n",
              "      <td>0.500398</td>\n",
              "      <td>83.488903</td>\n",
              "      <td>83.811166</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.355798</td>\n",
              "      <td>0.572516</td>\n",
              "      <td>0.526975</td>\n",
              "      <td>83.322962</td>\n",
              "      <td>83.746497</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>256.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.560836</td>\n",
              "      <td>0.511553</td>\n",
              "      <td>83.281477</td>\n",
              "      <td>83.681828</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>512.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.333919</td>\n",
              "      <td>0.681067</td>\n",
              "      <td>0.629749</td>\n",
              "      <td>81.953951</td>\n",
              "      <td>82.884242</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.381044</td>\n",
              "      <td>0.615315</td>\n",
              "      <td>0.550612</td>\n",
              "      <td>81.352417</td>\n",
              "      <td>82.798017</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.163390</td>\n",
              "      <td>0.450630</td>\n",
              "      <td>0.408758</td>\n",
              "      <td>87.699647</td>\n",
              "      <td>88.445786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.146835</td>\n",
              "      <td>0.585115</td>\n",
              "      <td>0.537446</td>\n",
              "      <td>86.455092</td>\n",
              "      <td>87.001509</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>256.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.148623</td>\n",
              "      <td>0.614594</td>\n",
              "      <td>0.551444</td>\n",
              "      <td>86.206181</td>\n",
              "      <td>86.893727</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>512.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.196667</td>\n",
              "      <td>0.687148</td>\n",
              "      <td>0.648792</td>\n",
              "      <td>84.546775</td>\n",
              "      <td>85.708127</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.179426</td>\n",
              "      <td>0.687404</td>\n",
              "      <td>0.660889</td>\n",
              "      <td>84.422319</td>\n",
              "      <td>85.471007</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d0ba17c-8e7d-4aa0-b182-fc4b12b96636')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d0ba17c-8e7d-4aa0-b182-fc4b12b96636 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d0ba17c-8e7d-4aa0-b182-fc4b12b96636');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-117fe469-135c-43dd-90c4-680a56858fff\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-117fe469-135c-43dd-90c4-680a56858fff')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-117fe469-135c-43dd-90c4-680a56858fff button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Experiment with different nonlinearities(Identity, tanh, ReLU, Sigmoid) with number of hidden layers = 1, hidden layer of width = 128"
      ],
      "metadata": {
        "id": "gbz8wz6Tljva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_activation(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index, activation_function):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation_function == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation_function == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation_function == 'identity':\n",
        "            self.activation = lambda x: x\n",
        "        else:\n",
        "            self.activation = nn.Tanh()  # Default is tanh\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = self.activation(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "8orv1lGejuOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = []\n",
        "\n",
        "# Define configurations based on window sizes and activation functions\n",
        "window_sizes = [0, 1]\n",
        "activation_functions = ['identity', 'tanh', 'relu', 'sigmoid']\n",
        "\n",
        "for w in window_sizes:\n",
        "    for activation in activation_functions:\n",
        "        configs.append({\n",
        "            'window_size': w,\n",
        "            'activation_function': activation\n",
        "        })\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\\nTraining with context window size w={config['window_size']} and {config['activation_function']} activation function\")\n",
        "\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "\n",
        "    POST = POSTagger_embed(w=config['window_size'], vocab_size=len(embedding_df))\n",
        "    model = NN_activation(POST, pretrained_embeddings, twitter_to_index, activation_function=config['activation_function'])\n",
        "\n",
        "    model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "\n",
        "    result = model.fit()\n",
        "\n",
        "    if isinstance(result, tuple):\n",
        "        result = {\n",
        "            'train_loss': result[0],\n",
        "            'dev_loss': result[1],\n",
        "            'devtest_loss': result[2],\n",
        "            'dev_accuracy': result[3],\n",
        "            'devtest_accuracy': result[4]\n",
        "        }\n",
        "\n",
        "    result.update(config)\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "6mhr3ROAqzny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c993d408-cca5-401b-e74e-5839bfde62f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0 and identity activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0206\n",
            "Loss: 0.6005, Accuracy: 82.7837%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5117\n",
            "Loss: 0.6419, Accuracy: 82.8459%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4417\n",
            "Loss: 0.5462, Accuracy: 84.2771%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4072\n",
            "Loss: 0.5897, Accuracy: 82.0784%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3897\n",
            "Loss: 0.6058, Accuracy: 83.9245%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3784\n",
            "Loss: 0.6099, Accuracy: 80.8961%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3696\n",
            "Loss: 0.5690, Accuracy: 82.6177%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3658\n",
            "Loss: 0.5713, Accuracy: 82.6177%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5394, Accuracy: 82.7980%\n",
            "\n",
            "Training with context window size w=0 and tanh activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0284\n",
            "Loss: 0.5884, Accuracy: 83.3022%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5124\n",
            "Loss: 0.5970, Accuracy: 82.7629%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4400\n",
            "Loss: 0.5674, Accuracy: 83.8000%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4060\n",
            "Loss: 0.5493, Accuracy: 84.1942%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3868\n",
            "Loss: 0.5497, Accuracy: 84.0075%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3766\n",
            "Loss: 0.5653, Accuracy: 82.6799%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3638\n",
            "Loss: 0.5774, Accuracy: 83.9245%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3608\n",
            "Loss: 0.5720, Accuracy: 83.1985%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3525\n",
            "Loss: 0.5702, Accuracy: 83.4059%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5156, Accuracy: 84.2207%\n",
            "\n",
            "Training with context window size w=0 and relu activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0755\n",
            "Loss: 0.6076, Accuracy: 83.4474%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5060\n",
            "Loss: 0.5513, Accuracy: 84.0282%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4195\n",
            "Loss: 0.5497, Accuracy: 83.8415%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3848\n",
            "Loss: 0.5493, Accuracy: 83.3644%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3637\n",
            "Loss: 0.5498, Accuracy: 84.1942%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3517\n",
            "Loss: 0.5572, Accuracy: 83.9245%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3431\n",
            "Loss: 0.5615, Accuracy: 84.3601%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3338\n",
            "Loss: 0.5552, Accuracy: 84.3393%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3299\n",
            "Loss: 0.5841, Accuracy: 83.7378%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5407, Accuracy: 83.4232%\n",
            "\n",
            "Training with context window size w=0 and sigmoid activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 2.2743\n",
            "Loss: 1.1504, Accuracy: 68.2224%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.8518\n",
            "Loss: 0.7486, Accuracy: 79.6515%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5869\n",
            "Loss: 0.6691, Accuracy: 81.1242%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4761\n",
            "Loss: 0.6365, Accuracy: 82.3273%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4285\n",
            "Loss: 0.6717, Accuracy: 81.6636%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.4018\n",
            "Loss: 0.6471, Accuracy: 82.2029%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3856\n",
            "Loss: 0.6072, Accuracy: 83.0741%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3731\n",
            "Loss: 0.6036, Accuracy: 83.4889%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3621\n",
            "Loss: 0.6218, Accuracy: 82.7007%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3568\n",
            "Loss: 0.6245, Accuracy: 82.7422%\n",
            "------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3518\n",
            "Loss: 0.6113, Accuracy: 83.2607%\n",
            "------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3446\n",
            "Loss: 0.5978, Accuracy: 83.5511%\n",
            "------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.3413\n",
            "Loss: 0.5933, Accuracy: 83.5511%\n",
            "------------------------------------\n",
            "Epoch 13\n",
            "Epoch 13, Loss: 0.3354\n",
            "Loss: 0.6055, Accuracy: 83.2815%\n",
            "------------------------------------\n",
            "Epoch 14\n",
            "Epoch 14, Loss: 0.3334\n",
            "Loss: 0.5974, Accuracy: 83.4059%\n",
            "------------------------------------\n",
            "Epoch 15\n",
            "Epoch 15, Loss: 0.3298\n",
            "Loss: 0.6195, Accuracy: 83.3852%\n",
            "------------------------------------\n",
            "Epoch 16\n",
            "Epoch 16, Loss: 0.3271\n",
            "Loss: 0.5995, Accuracy: 83.5511%\n",
            "------------------------------------\n",
            "Epoch 17\n",
            "Epoch 17, Loss: 0.3234\n",
            "Loss: 0.6014, Accuracy: 83.6548%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5491, Accuracy: 84.3501%\n",
            "\n",
            "Training with context window size w=1 and identity activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9120\n",
            "Loss: 0.5097, Accuracy: 85.4180%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4033\n",
            "Loss: 0.4972, Accuracy: 85.9158%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3064\n",
            "Loss: 0.5002, Accuracy: 86.3099%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2483\n",
            "Loss: 0.5314, Accuracy: 85.9780%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2042\n",
            "Loss: 0.5536, Accuracy: 86.0610%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1812\n",
            "Loss: 0.5695, Accuracy: 86.4343%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1600\n",
            "Loss: 0.5998, Accuracy: 86.1854%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5573, Accuracy: 86.8291%\n",
            "\n",
            "Training with context window size w=1 and tanh activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9180\n",
            "Loss: 0.5296, Accuracy: 85.1068%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4039\n",
            "Loss: 0.4979, Accuracy: 85.6254%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3010\n",
            "Loss: 0.4904, Accuracy: 87.0151%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2436\n",
            "Loss: 0.4990, Accuracy: 87.1603%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2022\n",
            "Loss: 0.5387, Accuracy: 86.7455%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1697\n",
            "Loss: 0.5605, Accuracy: 86.9529%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1450\n",
            "Loss: 0.5761, Accuracy: 86.3721%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1249\n",
            "Loss: 0.6292, Accuracy: 86.3721%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5868, Accuracy: 86.5273%\n",
            "\n",
            "Training with context window size w=1 and relu activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9571\n",
            "Loss: 0.5171, Accuracy: 85.1691%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3976\n",
            "Loss: 0.4704, Accuracy: 86.9737%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2845\n",
            "Loss: 0.5010, Accuracy: 86.6625%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2219\n",
            "Loss: 0.5162, Accuracy: 86.6833%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1781\n",
            "Loss: 0.5270, Accuracy: 87.1603%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1452\n",
            "Loss: 0.5642, Accuracy: 86.8492%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1222\n",
            "Loss: 0.5913, Accuracy: 86.3099%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5472, Accuracy: 87.0877%\n",
            "\n",
            "Training with context window size w=1 and sigmoid activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 2.0808\n",
            "Loss: 0.9809, Accuracy: 71.8316%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.7211\n",
            "Loss: 0.6159, Accuracy: 83.4474%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4754\n",
            "Loss: 0.5372, Accuracy: 84.9824%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3633\n",
            "Loss: 0.5232, Accuracy: 85.8743%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2958\n",
            "Loss: 0.5178, Accuracy: 86.1647%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2537\n",
            "Loss: 0.5181, Accuracy: 86.5173%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.2235\n",
            "Loss: 0.5209, Accuracy: 86.9737%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1969\n",
            "Loss: 0.5416, Accuracy: 86.5381%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.1780\n",
            "Loss: 0.5331, Accuracy: 86.9322%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.1615\n",
            "Loss: 0.5733, Accuracy: 86.5381%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5168, Accuracy: 87.7775%\n",
            "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.365771  0.571287      0.539360     82.617714         82.798017   \n",
            "1    0.352529  0.570157      0.515562     83.405932         84.220737   \n",
            "2    0.329943  0.584074      0.540747     83.737814         83.423152   \n",
            "3    0.323410  0.601395      0.549057     83.654843         84.350075   \n",
            "4    0.159980  0.599824      0.557334     86.185439         86.829058   \n",
            "5    0.124933  0.629154      0.586767     86.372122         86.527269   \n",
            "6    0.122173  0.591303      0.547208     86.309894         87.087734   \n",
            "7    0.161495  0.573327      0.516840     86.538063         87.777538   \n",
            "\n",
            "   window_size activation_function  \n",
            "0            0            identity  \n",
            "1            0                tanh  \n",
            "2            0                relu  \n",
            "3            0             sigmoid  \n",
            "4            1            identity  \n",
            "5            1                tanh  \n",
            "6            1                relu  \n",
            "7            1             sigmoid  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overly, ReLU or Sigmoid produce the highest test accuracy, but it doesn't differ that much."
      ],
      "metadata": {
        "id": "MsBgC90OLgyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "4diGXfditMFQ",
        "outputId": "7c57bdcf-486b-4e8a-fadb-c0b8797a9cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.365771  0.571287      0.539360     82.617714         82.798017   \n",
              "1    0.352529  0.570157      0.515562     83.405932         84.220737   \n",
              "2    0.329943  0.584074      0.540747     83.737814         83.423152   \n",
              "3    0.323410  0.601395      0.549057     83.654843         84.350075   \n",
              "4    0.159980  0.599824      0.557334     86.185439         86.829058   \n",
              "5    0.124933  0.629154      0.586767     86.372122         86.527269   \n",
              "6    0.122173  0.591303      0.547208     86.309894         87.087734   \n",
              "7    0.161495  0.573327      0.516840     86.538063         87.777538   \n",
              "\n",
              "   window_size activation_function  \n",
              "0            0            identity  \n",
              "1            0                tanh  \n",
              "2            0                relu  \n",
              "3            0             sigmoid  \n",
              "4            1            identity  \n",
              "5            1                tanh  \n",
              "6            1                relu  \n",
              "7            1             sigmoid  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10ed9eac-0f70-439e-a72c-0eb7e9a9fc53\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "      <th>activation_function</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.365771</td>\n",
              "      <td>0.571287</td>\n",
              "      <td>0.539360</td>\n",
              "      <td>82.617714</td>\n",
              "      <td>82.798017</td>\n",
              "      <td>0</td>\n",
              "      <td>identity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.352529</td>\n",
              "      <td>0.570157</td>\n",
              "      <td>0.515562</td>\n",
              "      <td>83.405932</td>\n",
              "      <td>84.220737</td>\n",
              "      <td>0</td>\n",
              "      <td>tanh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.329943</td>\n",
              "      <td>0.584074</td>\n",
              "      <td>0.540747</td>\n",
              "      <td>83.737814</td>\n",
              "      <td>83.423152</td>\n",
              "      <td>0</td>\n",
              "      <td>relu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.323410</td>\n",
              "      <td>0.601395</td>\n",
              "      <td>0.549057</td>\n",
              "      <td>83.654843</td>\n",
              "      <td>84.350075</td>\n",
              "      <td>0</td>\n",
              "      <td>sigmoid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.159980</td>\n",
              "      <td>0.599824</td>\n",
              "      <td>0.557334</td>\n",
              "      <td>86.185439</td>\n",
              "      <td>86.829058</td>\n",
              "      <td>1</td>\n",
              "      <td>identity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.124933</td>\n",
              "      <td>0.629154</td>\n",
              "      <td>0.586767</td>\n",
              "      <td>86.372122</td>\n",
              "      <td>86.527269</td>\n",
              "      <td>1</td>\n",
              "      <td>tanh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.122173</td>\n",
              "      <td>0.591303</td>\n",
              "      <td>0.547208</td>\n",
              "      <td>86.309894</td>\n",
              "      <td>87.087734</td>\n",
              "      <td>1</td>\n",
              "      <td>relu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.161495</td>\n",
              "      <td>0.573327</td>\n",
              "      <td>0.516840</td>\n",
              "      <td>86.538063</td>\n",
              "      <td>87.777538</td>\n",
              "      <td>1</td>\n",
              "      <td>sigmoid</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10ed9eac-0f70-439e-a72c-0eb7e9a9fc53')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-10ed9eac-0f70-439e-a72c-0eb7e9a9fc53 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-10ed9eac-0f70-439e-a72c-0eb7e9a9fc53');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a26697dd-deef-4552-a30d-b0bc138c73f4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a26697dd-deef-4552-a30d-b0bc138c73f4')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a26697dd-deef-4552-a30d-b0bc138c73f4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Experiment with w = 2 and compare the results to w = 0 and 1"
      ],
      "metadata": {
        "id": "Ty3sB4vNllvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [{'window_size': w} for w in [0, 1, 2]]\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\\nTraining with context window size w={config['window_size']}\")\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    POST = POSTagger_embed(w=config['window_size'], vocab_size=len(embedding_df))\n",
        "    model = NN_embed(POST, pretrained_embeddings, twitter_to_index)\n",
        "    model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "    result = model.fit()\n",
        "\n",
        "    if isinstance(result, tuple):\n",
        "        result = {\n",
        "            'train_loss': result[0],\n",
        "            'dev_loss': result[1],\n",
        "            'devtest_loss': result[2],\n",
        "            'dev_accuracy': result[3],\n",
        "            'devtest_accuracy': result[4]\n",
        "        }\n",
        "\n",
        "    result.update(config)\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2001
        },
        "id": "NxuldLnflU2W",
        "outputId": "31e82b66-4527-4bb1-8814-a756e52fb001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0211\n",
            "Loss: 0.5987, Accuracy: 83.0948%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5131\n",
            "Loss: 0.5488, Accuracy: 82.9496%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4440\n",
            "Loss: 0.5785, Accuracy: 83.2192%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4084\n",
            "Loss: 0.5443, Accuracy: 83.3437%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3885\n",
            "Loss: 0.6181, Accuracy: 81.9747%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3764\n",
            "Loss: 0.5634, Accuracy: 84.0904%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3690\n",
            "Loss: 0.5886, Accuracy: 83.7793%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3647\n",
            "Loss: 0.5692, Accuracy: 83.4267%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3579\n",
            "Loss: 0.5508, Accuracy: 83.7586%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5141, Accuracy: 84.1345%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9213\n",
            "Loss: 0.5266, Accuracy: 85.2728%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4033\n",
            "Loss: 0.4835, Accuracy: 86.6003%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3018\n",
            "Loss: 0.5139, Accuracy: 86.5795%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2442\n",
            "Loss: 0.5466, Accuracy: 86.1854%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2017\n",
            "Loss: 0.5388, Accuracy: 86.5173%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1737\n",
            "Loss: 0.6091, Accuracy: 86.2684%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1473\n",
            "Loss: 0.5659, Accuracy: 86.5588%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5281, Accuracy: 87.2386%\n",
            "\n",
            "Training with context window size w=2\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9172\n",
            "Loss: 0.5176, Accuracy: 85.3143%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3919\n",
            "Loss: 0.4808, Accuracy: 86.0195%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2819\n",
            "Loss: 0.5070, Accuracy: 85.8121%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2109\n",
            "Loss: 0.5463, Accuracy: 86.0195%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1570\n",
            "Loss: 0.5771, Accuracy: 85.9573%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1137\n",
            "Loss: 0.6321, Accuracy: 85.7084%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.0925\n",
            "Loss: 0.6768, Accuracy: 85.5632%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.6119, Accuracy: 86.6997%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.357889  0.550782      0.514058     83.758556         84.134512   \n",
              "1    0.147316  0.565908      0.528085     86.558805         87.238629   \n",
              "2    0.092491  0.676776      0.611927     85.563161         86.699720   \n",
              "\n",
              "   window_size  \n",
              "0            0  \n",
              "1            1  \n",
              "2            2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb1156ff-32fb-4e16-ba6e-47123a80d7f3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.357889</td>\n",
              "      <td>0.550782</td>\n",
              "      <td>0.514058</td>\n",
              "      <td>83.758556</td>\n",
              "      <td>84.134512</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.147316</td>\n",
              "      <td>0.565908</td>\n",
              "      <td>0.528085</td>\n",
              "      <td>86.558805</td>\n",
              "      <td>87.238629</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.092491</td>\n",
              "      <td>0.676776</td>\n",
              "      <td>0.611927</td>\n",
              "      <td>85.563161</td>\n",
              "      <td>86.699720</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb1156ff-32fb-4e16-ba6e-47123a80d7f3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb1156ff-32fb-4e16-ba6e-47123a80d7f3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb1156ff-32fb-4e16-ba6e-47123a80d7f3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a80086c8-b08b-459d-b4df-755687c44d1e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a80086c8-b08b-459d-b4df-755687c44d1e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a80086c8-b08b-459d-b4df-755687c44d1e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window size 1 produces the best test accuracy, and we can also see that the bigger window size does not make the test accuracy higher.\n"
      ],
      "metadata": {
        "id": "HfO9_ImrL5Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "vf04w9kSuA1m",
        "outputId": "3549ad20-96ca-432d-ba1a-ae7a383aeafb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.357889  0.550782      0.514058     83.758556         84.134512   \n",
              "1    0.147316  0.565908      0.528085     86.558805         87.238629   \n",
              "2    0.092491  0.676776      0.611927     85.563161         86.699720   \n",
              "\n",
              "   window_size  \n",
              "0            0  \n",
              "1            1  \n",
              "2            2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed4e3638-3f1e-431c-8ec7-9f7c83f05a23\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.357889</td>\n",
              "      <td>0.550782</td>\n",
              "      <td>0.514058</td>\n",
              "      <td>83.758556</td>\n",
              "      <td>84.134512</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.147316</td>\n",
              "      <td>0.565908</td>\n",
              "      <td>0.528085</td>\n",
              "      <td>86.558805</td>\n",
              "      <td>87.238629</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.092491</td>\n",
              "      <td>0.676776</td>\n",
              "      <td>0.611927</td>\n",
              "      <td>85.563161</td>\n",
              "      <td>86.699720</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed4e3638-3f1e-431c-8ec7-9f7c83f05a23')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed4e3638-3f1e-431c-8ec7-9f7c83f05a23 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed4e3638-3f1e-431c-8ec7-9f7c83f05a23');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-763d5fbd-6163-4f72-9518-bf9160c39de5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-763d5fbd-6163-4f72-9518-bf9160c39de5')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-763d5fbd-6163-4f72-9518-bf9160c39de5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.5 RNN Taggers"
      ],
      "metadata": {
        "id": "H1UVKLqumYTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger_Additional:\n",
        "  def __init__(self, w, vocab_size):\n",
        "    self.w = w\n",
        "    self.dim_e = 50\n",
        "    self.dim_h = 512\n",
        "    self.dim_s = len(tag_to_ix)\n",
        "    self.vocab_size = len(twitter_to_index)\n",
        "    self.batch_size = 32\n",
        "    self.epochs = 20"
      ],
      "metadata": {
        "id": "0vtHtGNyTIVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_Additional(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index, rnn_type, bidirectional):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        direction_factor = 2 if bidirectional else 1\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h * direction_factor, self.POSTagger.dim_s)\n",
        "        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n",
        "        self.rnn = rnn_class(self.POSTagger.dim_e * (2 * self.POSTagger.w + 1), self.POSTagger.dim_h, batch_first=True, bidirectional=bidirectional)\n",
        "        # Output layer\n",
        "\n",
        "        self.dropout_first = nn.Dropout(0.5)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        \"\"\"\n",
        "        Initialize the embedding layer with pretrained embeddings.\n",
        "        Words not found in the pretrained list will be initialized with the UUUNKKK vector.\n",
        "        \"\"\"\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "     #   self.rnn.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        embeds = self.dropout_first(embeds)\n",
        "        hidden_out = self.rnn(embeds)\n",
        "        hidden_out, _ = self.rnn(embeds)\n",
        "        hidden_activated = F.relu(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "iNtwnZFQlnny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POST = POSTagger_Additional(w=2, vocab_size = len(embedding_df))\n",
        "def train_and_evaluate_models(configurations):\n",
        "    results = []\n",
        "\n",
        "    for config in configurations:\n",
        "        rnn_type, bidirectional = config['rnn_type'], config['bidirectional']\n",
        "        print(f\"\\nTraining model with RNN type: {rnn_type} | Bidirectional: {str(bidirectional)}\")\n",
        "\n",
        "        model = NN_Additional(POST, pretrained_embeddings, twitter_to_index, rnn_type=rnn_type, bidirectional=bidirectional)\n",
        "\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "\n",
        "        train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy = model.fit()\n",
        "\n",
        "        results.append({\n",
        "            'rnn_type': rnn_type,\n",
        "            'bidirectional': bidirectional,\n",
        "            'train_loss': train_loss,\n",
        "            'dev_loss': dev_loss,\n",
        "            'devtest_loss': devtest_loss,\n",
        "            'dev_accuracy': dev_accuracy,\n",
        "            'devtest_accuracy': devtest_accuracy\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "configurations = [\n",
        "    {'rnn_type': 'LSTM', 'bidirectional': True},\n",
        "    {'rnn_type': 'RNN', 'bidirectional': True},\n",
        "    {'rnn_type': 'GRU', 'bidirectional': True},\n",
        "    {'rnn_type': 'LSTM', 'bidirectional': False},\n",
        "    {'rnn_type': 'RNN', 'bidirectional': False},\n",
        "    {'rnn_type': 'GRU', 'bidirectional': False}\n",
        "]\n",
        "\n",
        "results_df = train_and_evaluate_models(configurations)\n",
        "\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "iv_l64ACJ_n1",
        "outputId": "47f2c792-d96f-44be-c2a3-f70d4dc5ab9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with RNN type: LSTM | Bidirectional: True\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 2.5168\n",
            "Loss: 2.4098, Accuracy: 32.2755%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 2.3367\n",
            "Loss: 2.3991, Accuracy: 37.8967%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 2.2384\n",
            "Loss: 2.4541, Accuracy: 34.3912%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 2.2067\n",
            "Loss: 2.3564, Accuracy: 35.3868%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 2.2297\n",
            "Loss: 2.4090, Accuracy: 34.1008%\n",
            "------------------------------------\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-8260bdfd25ee>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m ]\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigurations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-8260bdfd25ee>\u001b[0m in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m(configurations)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevtest_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwitter_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         results.append({\n",
            "\u001b[0;32m<ipython-input-61-e97ea9c861f5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-e97ea9c861f5>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(self, epoch, sentence, label)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-e97ea9c861f5>\u001b[0m in \u001b[0;36mrun_grad\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v6UuDOj9RsA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}