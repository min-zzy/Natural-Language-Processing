{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Install the necessary libraries\n"],"metadata":{"id":"57H-mVBq1d4v"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"QCdFoYit1Iis","executionInfo":{"status":"ok","timestamp":1701393916284,"user_tz":360,"elapsed":14600,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"outputs":[],"source":["%%capture\n","! pip install tqdm boto3 requests regex sentencepiece sacremoses\n","! pip install transformers"]},{"cell_type":"markdown","source":["## BERT Features\n","\n","In this part, you will use BERT features to classify DBPedia articles.\n","The data is already pre-processed, and the data loader is implemented below."],"metadata":{"id":"y2usTWPt2Tjs"}},{"cell_type":"code","source":["# Basics: dataset, data loaders, Classifier\n","import collections\n","import json\n","import torch\n","import torch.nn as nn\n","import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel\n","\n","\n","SPLITS = ['train', 'dev', 'test']\n","\n","class DBPediaDataset(Dataset):\n","  '''DBPedia dataset.\n","    Args:\n","      path[str]: path to the original data.\n","  '''\n","  def __init__(self, path):\n","    with open(path) as fin:\n","      self._data = [json.loads(l) for l in fin]\n","    self._n_classes = len(set([datum['label'] for datum in self._data]))\n","\n","  def __getitem__(self, index):\n","    return self._data[index]\n","\n","  def __len__(self):\n","    return len(self._data)\n","\n","  @property\n","  def n_classes(self):\n","    return self._n_classes\n","\n","  @staticmethod\n","  def collate_fn(tokenizer, device, batch):\n","    '''The collate function that compresses a training batch.\n","      Args:\n","        batch[list[dict[str, Any]]]: data in the batch.\n","      Returns:\n","        labels[torch.LongTensor]: the labels in the batch.\n","        sentences[dict[str, torch.Tensor]]: sentences converted by tokenizers.\n","    '''\n","    labels = torch.tensor([datum['label'] for datum in batch]).long().to(device)\n","    sentences = tokenizer(\n","        [datum['sentence'] for datum in batch],\n","        return_tensors='pt',  # pt = pytorch style tensor\n","        padding=True)\n","    for key in sentences:\n","      sentences[key] = sentences[key].to(device)\n","    return labels, sentences\n","\n","def construct_datasets(prefix, batch_size, tokenizer, device):\n","  '''Constructs datasets and data loaders.\n","    Args:\n","      prefix[str]: prefix of the dataset (e.g., dbpedia_).\n","      batch_size[int]: maximum number of examples in a batch.\n","      tokenizer: model tokenizer that converts sentences to integer tensors.\n","      device[torch.device]: the device (cpu/gpu) that the tensor should be on.\n","    Returns:\n","      datasets[dict[str, Dataset]]: a dict of constructed datasets.\n","      dataloaders[dict[str, DataLoader]]: a dict of constructed data loaders.\n","  '''\n","  datasets = collections.defaultdict()\n","  dataloaders = collections.defaultdict()\n","  for split in SPLITS:\n","    datasets[split] = DBPediaDataset(f'{prefix}{split}.json')\n","    dataloaders[split] = DataLoader(\n","        datasets[split],\n","        batch_size=batch_size,\n","        shuffle=(split == 'train'),\n","        collate_fn=lambda x:DBPediaDataset.collate_fn(tokenizer, device, x))\n","  return datasets, dataloaders"],"metadata":{"id":"Sr_s2OH017B3","executionInfo":{"status":"ok","timestamp":1701393916285,"user_tz":360,"elapsed":5,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","import numpy as np"],"metadata":{"id":"ErNSdJzAVGGD","executionInfo":{"status":"ok","timestamp":1701393916285,"user_tz":360,"elapsed":4,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Classifier(nn.Module):\n","    def __init__(self, input_dim, hidden_size, n_classes):\n","        super(Classifier, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, n_classes)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"h6PlR_UXpSUk","executionInfo":{"status":"ok","timestamp":1701365885476,"user_tz":360,"elapsed":240,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Training and Evaluation"],"metadata":{"id":"isBytE8_OUSX"}},{"cell_type":"markdown","source":["# **1.1**"],"metadata":{"id":"PAU3_XO-d4h0"}},{"cell_type":"code","source":["print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYy06dGu1qnT","executionInfo":{"status":"ok","timestamp":1701365886891,"user_tz":360,"elapsed":248,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"25059710-1cdf-4645-e257-b3d898a2d5c2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["def set_seed(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","def train_model(dataloaders, classifier, optimizer, loss_func):\n","    pbar = tqdm.tqdm(dataloaders['train'])\n","    for labels, sentences in pbar:\n","        with torch.no_grad():\n","            unpooled_features = bert_model(**sentences)['last_hidden_state']\n","        cls_token = unpooled_features[:, 0, :]\n","        outputs = classifier(cls_token)\n","        loss = loss_func(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")"],"metadata":{"id":"9aiKTxQVWmcH","executionInfo":{"status":"ok","timestamp":1701365887199,"user_tz":360,"elapsed":0,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, dataloaders, bert_model, is_test=False):\n","    model.eval()\n","    dataloader = dataloaders['test'] if is_test else dataloaders['dev']\n","\n","    total, correct, total_loss = 0, 0, 0.0\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    with torch.no_grad():\n","        for labels, sentences in dataloader:\n","            unpooled_features = bert_model(**sentences)['last_hidden_state']\n","            cls_token = unpooled_features[:, 0, :]\n","            outputs = model(cls_token)\n","\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    average_loss = total_loss / len(dataloader)\n","    accuracy = 100 * correct / total\n","    return accuracy, average_loss"],"metadata":{"id":"-uC0-LhpphHg","executionInfo":{"status":"ok","timestamp":1701365974099,"user_tz":360,"elapsed":1,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","batch_size = 32\n","classifier_hidden_size = 32\n","\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n","bert_model = AutoModel.from_pretrained('bert-base-cased')\n","if torch.cuda.is_available():\n","    bert_model = bert_model.cuda()\n","\n","datasets, dataloaders = construct_datasets(prefix='dbpedia_', batch_size=batch_size, tokenizer=tokenizer, device=bert_model.device)"],"metadata":{"id":"EHXEP72zQbbP","executionInfo":{"status":"ok","timestamp":1701365975970,"user_tz":360,"elapsed":1521,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["dev_accuracies = []\n","best_model = None\n","best_loss = float('inf')\n","\n","for seed in range(5):\n","    set_seed(seed)\n","    classifier = Classifier(bert_model.config.hidden_size, classifier_hidden_size, datasets['train'].n_classes).to(bert_model.device)\n","    optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    train_model(dataloaders, classifier, optimizer, loss_func)\n","    accuracy, loss = evaluate(classifier, dataloaders, bert_model)\n","    dev_accuracies.append(accuracy)\n","\n","    if loss < best_loss:\n","        best_loss = loss\n","        best_model = classifier\n","\n","# Evaluate the best model on the test set\n","test_accuracy, test_loss = evaluate(best_model, dataloaders, bert_model, is_test=True)\n","\n","# Calculate mean and standard deviation\n","mean_accuracy = np.mean(dev_accuracies)\n","std_dev_accuracy = np.std(dev_accuracies)\n","\n","print(f\"Mean Dev Accuracy: {mean_accuracy}\")\n","print(f\"Standard Deviation: {std_dev_accuracy}\")\n","print(f\"Test Set Accuracy of Best Model: {test_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnsSr0mpWxni","executionInfo":{"status":"ok","timestamp":1701366308535,"user_tz":360,"elapsed":79337,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"0947e396-8b84-4935-86cd-40222919fafe"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Loss: 0.3805: 100%|██████████| 313/313 [00:14<00:00, 22.04it/s]\n","Loss: 0.4824: 100%|██████████| 313/313 [00:14<00:00, 22.15it/s]\n","Loss: 0.6758: 100%|██████████| 313/313 [00:14<00:00, 22.27it/s]\n","Loss: 0.3257: 100%|██████████| 313/313 [00:14<00:00, 21.76it/s]\n","Loss: 0.4815: 100%|██████████| 313/313 [00:14<00:00, 21.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Mean Dev Accuracy: 96.16\n","Standard Deviation: 0.952050418832951\n","Test Set Accuracy of Best Model: 97.7\n"]}]},{"cell_type":"markdown","source":["# **1.2**"],"metadata":{"id":"KI6e8JnDd-Nu"}},{"cell_type":"markdown","source":["1. Define Function"],"metadata":{"id":"IX3Uoo46worC"}},{"cell_type":"code","source":["def mean_pooling(token_embeddings, attention_mask):\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","    return sum_embeddings / sum_mask\n","\n","def max_pooling(token_embeddings, attention_mask):\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n","    max_values = torch.max(token_embeddings, 1).values  # Extracting only the values\n","    return max_values"],"metadata":{"id":"1zkKJCy6rnZG","executionInfo":{"status":"ok","timestamp":1701366314918,"user_tz":360,"elapsed":1,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def train_model_pooling(dataloaders, model, optimizer, loss_func, bert_model, pooling_method):\n","    model.train()\n","    for labels, data in tqdm.tqdm(dataloaders['train']):\n","        # Since data is already tokenized, extract the necessary fields directly\n","        input_ids = data['input_ids'].to(bert_model.device)\n","        attention_mask = data['attention_mask'].to(bert_model.device)\n","\n","        # Forward pass with BERT\n","        with torch.no_grad():\n","            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n","\n","        # Apply the specified pooling method\n","        if pooling_method == 'mean':\n","            pooled_outputs = mean_pooling(outputs, attention_mask)\n","        elif pooling_method == 'max':\n","            pooled_outputs = max_pooling(outputs, attention_mask)\n","\n","        # Classifier forward pass\n","        logits = model(pooled_outputs)\n","        loss = loss_func(logits, labels.to(bert_model.device))\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"id":"cXCAPyeZixKs","executionInfo":{"status":"ok","timestamp":1701366315229,"user_tz":360,"elapsed":2,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def evaluate_pooling(model, dataloaders, bert_model, pooling_method, is_test=False):\n","    model.eval()\n","    device = next(model.parameters()).device  # 모델 파라미터에서 디바이스 추출\n","    total, correct, total_loss = 0, 0, 0.0\n","\n","    # 교차 엔트로피 손실 함수 초기화\n","    criterion = nn.CrossEntropyLoss()\n","\n","    dataloader = dataloaders['test'] if is_test else dataloaders['dev']\n","    with torch.no_grad():\n","        for labels, data in dataloader:\n","            input_ids = data['input_ids'].to(device)\n","            attention_mask = data['attention_mask'].to(device)\n","\n","            # BERT 포워드 패스\n","            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n","\n","            # 지정된 풀링 방법 적용\n","            if pooling_method == 'mean':\n","                pooled_outputs = mean_pooling(outputs, attention_mask)\n","            elif pooling_method == 'max':\n","                pooled_outputs = max_pooling(outputs, attention_mask)\n","\n","            # 분류기 포워드 패스\n","            logits = model(pooled_outputs)\n","\n","            # 손실 계산\n","            loss = criterion(logits, labels.to(device))\n","            total_loss += loss.item()\n","\n","            _, predicted = torch.max(logits.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels.to(device)).sum().item()\n","\n","    accuracy = correct / total * 100\n","    average_loss = total_loss / len(dataloader)\n","    return accuracy, average_loss"],"metadata":{"id":"5MWtBt1ww5qS","executionInfo":{"status":"ok","timestamp":1701366316951,"user_tz":360,"elapsed":3,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["2. Mean Pooling"],"metadata":{"id":"0odzmjCxwrIy"}},{"cell_type":"code","source":["dev_accuracies = []\n","best_model = None\n","best_loss = float('inf')\n","\n","for seed in range(5):\n","    set_seed(seed)\n","    classifier = Classifier(bert_model.config.hidden_size, classifier_hidden_size, datasets['train'].n_classes).to(bert_model.device)\n","    optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    train_model_pooling(dataloaders, classifier, optimizer, loss_func, bert_model, 'mean')\n","    accuracy, loss = evaluate_pooling(classifier, dataloaders, bert_model, 'mean')\n","    dev_accuracies.append(accuracy)\n","\n","    if loss < best_loss:\n","        best_loss = loss\n","        best_model = classifier\n","\n","# Evaluate the best model on the test set\n","test_accuracy, test_loss = evaluate_pooling(best_model, dataloaders, bert_model, 'mean', is_test=True)\n","\n","# Calculate mean and standard deviation\n","mean_accuracy = np.mean(dev_accuracies)\n","std_dev_accuracy = np.std(dev_accuracies)\n","\n","print(f\"Mean Dev Accuracy: {mean_accuracy}\")\n","print(f\"Standard Deviation: {std_dev_accuracy}\")\n","print(f\"Test Set Accuracy of Best Model: {test_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4W_F2d5hlrk","executionInfo":{"status":"ok","timestamp":1701366392486,"user_tz":360,"elapsed":73622,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"e2c9af0a-6f55-4f39-ec5f-508c42438c99"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 313/313 [00:13<00:00, 23.47it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.59it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.56it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.28it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Mean Dev Accuracy: 96.96000000000001\n","Standard Deviation: 0.21540659228537873\n","Test Set Accuracy of Best Model: 96.7\n"]}]},{"cell_type":"markdown","source":["3. Max Pooling"],"metadata":{"id":"eTv7-hU69XBg"}},{"cell_type":"code","source":["dev_accuracies = []\n","best_model = None\n","best_loss = float('inf')\n","\n","for seed in range(5):\n","    set_seed(seed)\n","    classifier = Classifier(bert_model.config.hidden_size, classifier_hidden_size, datasets['train'].n_classes).to(bert_model.device)\n","    optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    train_model_pooling(dataloaders, classifier, optimizer, loss_func, bert_model, 'max')\n","    accuracy, loss = evaluate_pooling(classifier, dataloaders, bert_model, 'max')\n","    dev_accuracies.append(accuracy)\n","\n","    if loss < best_loss:\n","        best_loss = loss\n","        best_model = classifier\n","\n","# Evaluate the best model on the test set\n","test_accuracy, test_loss = evaluate_pooling(best_model, dataloaders, bert_model, 'max', is_test=True)\n","\n","# Calculate mean and standard deviation\n","mean_accuracy = np.mean(dev_accuracies)\n","std_dev_accuracy = np.std(dev_accuracies)\n","\n","print(f\"Mean Dev Accuracy: {mean_accuracy}\")\n","print(f\"Standard Deviation: {std_dev_accuracy}\")\n","print(f\"Test Set Accuracy of Best Model: {test_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0N8iBzEwweDc","executionInfo":{"status":"ok","timestamp":1701366519539,"user_tz":360,"elapsed":73942,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"7e2598d3-95a7-4554-f13d-a0417406550f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 313/313 [00:13<00:00, 23.51it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.58it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.50it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.31it/s]\n","100%|██████████| 313/313 [00:13<00:00, 23.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Mean Dev Accuracy: 65.17999999999999\n","Standard Deviation: 4.676494413553813\n","Test Set Accuracy of Best Model: 68.2\n"]}]},{"cell_type":"markdown","source":["# **1.3**"],"metadata":{"id":"tmHHW-hDOMJt"}},{"cell_type":"markdown","source":["1. **First-Token Pooling**:\n","\n","Mean Dev Accuracy: 96.16%\n","\n","Standard Deviation: 0.952\n","\n","Test Set Accuracy: 97.7%\n","\n","2. **Mean Pooling**:\n","\n","Mean Dev Accuracy: 96.96%\n","\n","Standard Deviation: 0.215\n","\n","Test Set Accuracy: 96.7%\n","\n","3. **Max Pooling**:\n","\n","Mean Dev Accuracy: 65.18%\n","\n","Standard Deviation: 4.676\n","\n","Test Set Accuracy: 68.2%\n","\n","4. **Analysis:**\n","\n","1) **Mean Development Accuracy:** This metric indicates how well the model performs on average on the development set. Higher is better. In this aspect, **mean pooling performs the best (96.96%)**, closely followed by first-token pooling (96.16%). Max pooling lags significantly behind (65.18%).\n","\n","2) **Standard Deviation:** This metric measures the model's development set performance variability across different runs. Lower is better, as it indicates more consistency. Again, **mean pooling** shows the best performance with the lowest standard deviation (0.215), suggesting it is the most stable and consistent across runs.\n","\n","3) **Test Set Accuracy:** This metric evaluates how well the model generalizes to unseen data. Higher is better. **First-token pooling leads (97.7%)**, with mean pooling slightly behind (96.7%). Max pooling has a much lower performance (68.2%).\n","\n","\n","5. **Conclusion:**\n","\n","1) **Best Overall:** **Mean pooling** is the most effective method, considering it has the highest mean development accuracy and the lowest standard deviation, indicating high performance and consistency. Although its test set accuracy is slightly lower than that of the first-token pooling, the differences are minimal and mean pooling's superior performance in the other two metrics makes it the preferable choice.\n","\n","2) **Least Effective:** **Max pooling** is the least effective in this scenario, with significantly lower development and test accuracies and higher standard deviation."],"metadata":{"id":"yaCgoC8b-GpO"}},{"cell_type":"markdown","source":["# **1.4**"],"metadata":{"id":"n-jttE1nxwXb"}},{"cell_type":"code","source":["def train_model_layer(dataloaders, classifier, optimizer, loss_func):\n","    pbar = tqdm.tqdm(dataloaders['train'])\n","    for labels, sentences in pbar:\n","        unpooled_features = bert_model(**sentences)['last_hidden_state']\n","        cls_token = unpooled_features[:, 0, :]\n","        outputs = classifier(cls_token)\n","        loss = loss_func(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")"],"metadata":{"id":"0wlcgJj19jAp","executionInfo":{"status":"ok","timestamp":1701366657698,"user_tz":360,"elapsed":237,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def evaluate_layer(model, dataloaders, bert_model, is_test=False):\n","    model.eval()\n","    dataloader = dataloaders['test'] if is_test else dataloaders['dev']\n","    total, correct, total_loss = 0, 0, 0.0\n","\n","    # 교차 엔트로피 손실 함수 초기화\n","    criterion = nn.CrossEntropyLoss()\n","\n","    with torch.no_grad():\n","        for labels, sentences in dataloader:\n","            unpooled_features = bert_model(**sentences)['last_hidden_state']\n","            cls_token = unpooled_features[:, 0, :]\n","            outputs = model(cls_token)\n","\n","            # 손실 계산\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    average_loss = total_loss / len(dataloader)\n","    return accuracy, average_loss"],"metadata":{"id":"jjYDNj9iBG4h","executionInfo":{"status":"ok","timestamp":1701366657698,"user_tz":360,"elapsed":2,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["dev_accuracies = []\n","best_model = None\n","best_loss = float('inf')\n","\n","for seed in range(5):\n","    set_seed(seed)\n","    classifier = Classifier(bert_model.config.hidden_size, classifier_hidden_size, datasets['train'].n_classes).to(bert_model.device)\n","    params = list()\n","    for name, param in bert_model.named_parameters():\n","      if name.startswith(('encoder.layer.10', 'encoder.layer.11')):\n","        param.requires_grad = True\n","        params.append(param)\n","      else:\n","        param.requires_grad = False\n","    optimizer = torch.optim.Adam(params + list(classifier.parameters()), lr=5e-4)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    train_model_layer(dataloaders, classifier, optimizer, loss_func)\n","    accuracy, loss = evaluate_layer(classifier, dataloaders, bert_model)\n","    dev_accuracies.append(accuracy)\n","\n","    if loss < best_loss:\n","        best_loss = loss\n","        best_model = classifier\n","\n","# Evaluate the best model on the test set\n","test_accuracy, test_loss = evaluate(best_model, dataloaders, bert_model, is_test=True)\n","\n","# Calculate mean and standard deviation\n","mean_accuracy = np.mean(dev_accuracies)\n","std_dev_accuracy = np.std(dev_accuracies)\n","\n","print(f\"Mean Dev Accuracy: {mean_accuracy}\")\n","print(f\"Standard Deviation: {std_dev_accuracy}\")\n","print(f\"Test Set Accuracy of Best Model: {test_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NuznW7Tf75BC","executionInfo":{"status":"ok","timestamp":1701366764822,"user_tz":360,"elapsed":97594,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"3d5762d2-cb4a-4549-b651-c6ee12a17886"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["Loss: 0.0203: 100%|██████████| 313/313 [00:17<00:00, 17.42it/s]\n","Loss: 0.0090: 100%|██████████| 313/313 [00:18<00:00, 17.38it/s]\n","Loss: 0.0087: 100%|██████████| 313/313 [00:17<00:00, 17.42it/s]\n","Loss: 0.0044: 100%|██████████| 313/313 [00:18<00:00, 17.29it/s]\n","Loss: 0.0090: 100%|██████████| 313/313 [00:18<00:00, 17.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Mean Dev Accuracy: 99.11999999999999\n","Standard Deviation: 0.3544009029333898\n","Test Set Accuracy of Best Model: 98.4\n"]}]},{"cell_type":"markdown","source":["# **1.5**"],"metadata":{"id":"jBoxIgLDT29j"}},{"cell_type":"code","source":["from transformers import GPT2Model, GPT2Tokenizer\n","import copy\n","\n","gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","gpt_model = GPT2Model.from_pretrained('gpt2')\n","gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"],"metadata":{"id":"WArqcRd1P4zu","executionInfo":{"status":"ok","timestamp":1701396247283,"user_tz":360,"elapsed":2063,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    gpt_model = gpt_model.cuda()"],"metadata":{"id":"CS6vK4ehcYTT","executionInfo":{"status":"ok","timestamp":1701396332611,"user_tz":360,"elapsed":427,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["gpt_model.config.n_embd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFIS6hSanMVy","executionInfo":{"status":"ok","timestamp":1701366777566,"user_tz":360,"elapsed":566,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"41ced62f-8e26-43c6-981b-74d322293120"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["class GPTClassifier(nn.Module):\n","    def __init__(self, gpt_hidden_size, classifier_hidden_size, num_classes, dropout_rate=0.2):\n","        super(GPTClassifier, self).__init__()\n","        self.activation = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n","        self.linear1 = nn.Linear(gpt_hidden_size, classifier_hidden_size)\n","\n","        # Additional intermediate layer\n","        self.intermediate_size = classifier_hidden_size // 2\n","        self.linear_intermediate = nn.Linear(classifier_hidden_size, self.intermediate_size)\n","\n","        self.linear2 = nn.Linear(self.intermediate_size, num_classes)\n","\n","    def forward(self, unpooled_features, attention_mask):\n","        # Selecting the last non-padded element based on the attention mask\n","        last_non_padded_idx = attention_mask.sum(dim=1) - 1\n","        pooled_features = unpooled_features[torch.arange(unpooled_features.size(0)), last_non_padded_idx]\n","\n","        # Pass through the first linear layer, activation, and dropout\n","        x = self.linear1(pooled_features)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","\n","        # Pass through the additional intermediate layer, activation, and dropout\n","        x = self.linear_intermediate(x)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","\n","        # Final linear layer\n","        return self.linear2(x)"],"metadata":{"id":"bHTrlUjmUqMN","executionInfo":{"status":"ok","timestamp":1701395688013,"user_tz":360,"elapsed":239,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["def train_model_gpt(dataloaders, classifier, optimizer, loss_func, gpt_model):\n","    pbar = tqdm.tqdm(dataloaders['train'])\n","    for labels, sentences in pbar:\n","        # Assuming sentences is a dictionary with 'input_ids' and 'attention_mask'\n","        input_ids = sentences['input_ids'].to(gpt_model.device)\n","        attention_mask = sentences['attention_mask'].to(gpt_model.device)\n","        labels = labels.to(gpt_model.device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass through GPT model\n","        with torch.no_grad():\n","            outputs = gpt_model(input_ids, attention_mask=attention_mask)\n","            unpooled_features = outputs.last_hidden_state\n","\n","        # Forward pass through the classifier\n","        logits = classifier(unpooled_features, attention_mask)\n","\n","        loss = loss_func(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")"],"metadata":{"id":"k9aXptSIVa4U","executionInfo":{"status":"ok","timestamp":1701394348280,"user_tz":360,"elapsed":195,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def evaluate_gpt(model, dataloaders, gpt_model, is_test=False):\n","    model.eval()  # Set the model to evaluation mode\n","    dataloader = dataloaders['test'] if is_test else dataloaders['dev']\n","    total, correct, total_loss = 0, 0, 0.0\n","\n","    # Initialize the cross-entropy loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            labels, sentences = batch\n","            input_ids = sentences['input_ids'].to(gpt_model.device)\n","            attention_mask = sentences['attention_mask'].to(gpt_model.device)\n","            labels = labels.to(gpt_model.device)\n","\n","            # Forward pass through GPT model\n","            outputs = gpt_model(input_ids, attention_mask=attention_mask)\n","            unpooled_features = outputs.last_hidden_state\n","\n","            # Forward pass through the classifier\n","            logits = model(unpooled_features, attention_mask)\n","\n","            # Calculate loss\n","            loss = criterion(logits, labels)\n","            total_loss += loss.item()\n","\n","            _, predicted = torch.max(logits, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    average_loss = total_loss / len(dataloader)\n","    accuracy = 100 * correct / total\n","    return accuracy, average_loss"],"metadata":{"id":"SXzKdaMSWONP","executionInfo":{"status":"ok","timestamp":1701394349185,"user_tz":360,"elapsed":1,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class DBPediaDataset(Dataset):\n","  '''DBPedia dataset.\n","    Args:\n","      path[str]: path to the original data.\n","  '''\n","  def __init__(self, path):\n","    with open(path) as fin:\n","      self._data = [json.loads(l) for l in fin]\n","    self._n_classes = len(set([datum['label'] for datum in self._data]))\n","\n","  def __getitem__(self, index):\n","    return self._data[index]\n","\n","  def __len__(self):\n","    return len(self._data)\n","\n","  @property\n","  def n_classes(self):\n","    return self._n_classes\n","\n","  @staticmethod\n","  def collate_fn(tokenizer, device, batch):\n","    labels = torch.tensor([datum['label'] for datum in batch]).long().to(device)\n","    sentences = tokenizer(\n","        [datum['sentence'] for datum in batch],\n","        return_tensors='pt',  # pt = pytorch style tensor\n","        padding='longest',  # Pad to the longest sequence in the batch\n","        truncation=True  # Truncate to the model's max input length\n","    )\n","    for key in sentences:\n","        sentences[key] = sentences[key].to(device)\n","    return labels, sentences\n","\n","def construct_datasets(prefix, batch_size, tokenizer, device):\n","  '''Constructs datasets and data loaders.\n","    Args:\n","      prefix[str]: prefix of the dataset (e.g., dbpedia_).\n","      batch_size[int]: maximum number of examples in a batch.\n","      tokenizer: model tokenizer that converts sentences to integer tensors.\n","      device[torch.device]: the device (cpu/gpu) that the tensor should be on.\n","    Returns:\n","      datasets[dict[str, Dataset]]: a dict of constructed datasets.\n","      dataloaders[dict[str, DataLoader]]: a dict of constructed data loaders.\n","  '''\n","  datasets = collections.defaultdict()\n","  dataloaders = collections.defaultdict()\n","  for split in SPLITS:\n","    datasets[split] = DBPediaDataset(f'{prefix}{split}.json')\n","    dataloaders[split] = DataLoader(\n","        datasets[split],\n","        batch_size=batch_size,\n","        shuffle=(split == 'train'),\n","        collate_fn=lambda x:DBPediaDataset.collate_fn(tokenizer, device, x))\n","  return datasets, dataloaders"],"metadata":{"id":"AH6oNe2Xh23Q","executionInfo":{"status":"ok","timestamp":1701394351404,"user_tz":360,"elapsed":176,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","datasets, dataloaders = construct_datasets(prefix='dbpedia_', batch_size=batch_size, tokenizer=gpt_tokenizer, device=gpt_model.device)"],"metadata":{"id":"8ELQz9FCgsz1","executionInfo":{"status":"ok","timestamp":1701396578678,"user_tz":360,"elapsed":210,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["classifier_hidden_size = 256"],"metadata":{"id":"Ck59cfGZdCJ8","executionInfo":{"status":"ok","timestamp":1701396580456,"user_tz":360,"elapsed":192,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()"],"metadata":{"id":"zaKdCVtiKhsS","executionInfo":{"status":"ok","timestamp":1701396580605,"user_tz":360,"elapsed":3,"user":{"displayName":"Anna !","userId":"17731284774928303452"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","num_classes = 14\n","learning_rate = 5e-4\n","num_seed = 5\n","\n","# Initialize and Train the Model\n","dev_accuracies = []\n","best_model = None\n","best_loss = 100\n","\n","\n","for seed in range(num_seed):\n","    classifier = GPTClassifier(gpt_model.config.hidden_size, classifier_hidden_size, num_classes).to(gpt_model.device)\n","    optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n","    loss_func = nn.CrossEntropyLoss()\n","    train_model_gpt(dataloaders, classifier, optimizer, loss_func, gpt_model)\n","\n","    accuracy, loss = evaluate_gpt(classifier, dataloaders, gpt_model)\n","    print(accuracy, loss)\n","    dev_accuracies.append(accuracy)\n","\n","    if loss < best_loss:\n","        best_loss = loss\n","        best_model = classifier\n","        print('best')\n","\n","# Evaluate the Best Model on the Test Set\n","test_accuracy, test_loss = evaluate_gpt(best_model, dataloaders, gpt_model, is_test=True)\n","\n","# Results\n","mean_accuracy = np.mean(dev_accuracies)\n","std_dev_accuracy = np.std(dev_accuracies)\n","\n","print(f\"Mean Dev Accuracy: {mean_accuracy:.2f}%\")\n","print(f\"Standard Deviation: {std_dev_accuracy:.2f}\")\n","print(f\"Test Set Accuracy of Best Model: {test_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDaFTkGNZETX","executionInfo":{"status":"ok","timestamp":1701396941169,"user_tz":360,"elapsed":92709,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"558a5a32-7eef-47f1-ca19-f497967590b8"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stderr","text":["Loss: 0.1343: 100%|██████████| 313/313 [00:16<00:00, 18.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["95.9 0.18426849134266376\n","best\n"]},{"output_type":"stream","name":"stderr","text":["Loss: 0.3300: 100%|██████████| 313/313 [00:16<00:00, 18.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["94.3 0.19865347375161946\n"]},{"output_type":"stream","name":"stderr","text":["Loss: 0.0780: 100%|██████████| 313/313 [00:16<00:00, 18.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["95.2 0.1793770119547844\n","best\n"]},{"output_type":"stream","name":"stderr","text":["Loss: 0.1635: 100%|██████████| 313/313 [00:16<00:00, 18.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["94.7 0.19335326505824924\n"]},{"output_type":"stream","name":"stderr","text":["Loss: 0.2603: 100%|██████████| 313/313 [00:16<00:00, 18.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["95.2 0.1700316604692489\n","best\n","Mean Dev Accuracy: 95.06%\n","Standard Deviation: 0.54\n","Test Set Accuracy of Best Model: 96.50%\n"]}]},{"cell_type":"markdown","source":["# **Exporting the Notebook**"],"metadata":{"id":"915K64u8_6-M"}},{"cell_type":"code","source":["!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHyJi1GB_6QK","executionInfo":{"status":"ok","timestamp":1701396995004,"user_tz":360,"elapsed":53860,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"0d1c0e58-d1f3-4aef-ec3d-ef5ab3e8977f"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","pandoc is already the newest version (2.9.2.1-3ubuntu2).\n","pandoc set to manually installed.\n","The following additional packages will be installed:\n","  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n","  fonts-urw-base35 libapache-pom-java libcommons-logging-java libcommons-parent-java\n","  libfontbox-java libfontenc1 libgs9 libgs9-common libidn12 libijs-0.35 libjbig2dec0 libkpathsea6\n","  libpdfbox-java libptexenc1 libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1\n","  libzzip-0-13 lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems\n","  ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre\n","  texlive-base texlive-binaries texlive-fonts-recommended texlive-latex-base\n","  texlive-latex-recommended texlive-pictures texlive-plain-generic tipa xfonts-encodings\n","  xfonts-utils\n","Suggested packages:\n","  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n","  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java poppler-utils ghostscript\n","  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n","  fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv\n","  | postscript-viewer perl-tk xpdf | pdf-viewer xzdec texlive-fonts-recommended-doc\n","  texlive-latex-base-doc python3-pygments icc-profiles libfile-which-perl\n","  libspreadsheet-parseexcel-perl texlive-latex-extra-doc texlive-latex-recommended-doc\n","  texlive-luatex texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex default-jre-headless\n","  tipa-doc\n","The following NEW packages will be installed:\n","  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n","  fonts-urw-base35 libapache-pom-java libcommons-logging-java libcommons-parent-java\n","  libfontbox-java libfontenc1 libgs9 libgs9-common libidn12 libijs-0.35 libjbig2dec0 libkpathsea6\n","  libpdfbox-java libptexenc1 libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1\n","  libzzip-0-13 lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems\n","  ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre texlive\n","  texlive-base texlive-binaries texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n","  texlive-latex-recommended texlive-pictures texlive-plain-generic texlive-xetex tipa\n","  xfonts-encodings xfonts-utils\n","0 upgraded, 55 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 182 MB of archives.\n","After this operation, 572 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.5 [752 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.5 [5,030 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.1 [60.3 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.1 [39.1 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.4 [50.1 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n","Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n","Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n","Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ruby-webrick all 1.7.0-3 [51.8 kB]\n","Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n","Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.4 [5,113 kB]\n","Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.1 [55.5 kB]\n","Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n","Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.1 [120 kB]\n","Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.1 [267 kB]\n","Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n","Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n","Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n","Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n","Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n","Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n","Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.1 [9,848 kB]\n","Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n","Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n","Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n","Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n","Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive all 2021.20220204-1 [14.3 kB]\n","Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n","Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n","Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n","Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n","Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n","Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n","Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n","Fetched 182 MB in 5s (37.9 MB/s)\n","Extracting templates from packages: 100%\n","Preconfiguring packages ...\n","Selecting previously unselected package fonts-droid-fallback.\n","(Reading database ... 120882 files and directories currently installed.)\n","Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n","Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Selecting previously unselected package fonts-lato.\n","Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n","Unpacking fonts-lato (2.0-2.1) ...\n","Selecting previously unselected package poppler-data.\n","Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n","Unpacking poppler-data (0.4.11-1) ...\n","Selecting previously unselected package tex-common.\n","Preparing to unpack .../03-tex-common_6.17_all.deb ...\n","Unpacking tex-common (6.17) ...\n","Selecting previously unselected package fonts-urw-base35.\n","Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n","Unpacking fonts-urw-base35 (20200910-1) ...\n","Selecting previously unselected package libgs9-common.\n","Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.5_all.deb ...\n","Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.5) ...\n","Selecting previously unselected package libidn12:amd64.\n","Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n","Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n","Selecting previously unselected package libijs-0.35:amd64.\n","Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n","Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n","Selecting previously unselected package libjbig2dec0:amd64.\n","Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n","Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n","Selecting previously unselected package libgs9:amd64.\n","Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.5_amd64.deb ...\n","Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.5) ...\n","Selecting previously unselected package libkpathsea6:amd64.\n","Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libwoff1:amd64.\n","Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n","Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n","Selecting previously unselected package dvisvgm.\n","Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n","Unpacking dvisvgm (2.13.1-1) ...\n","Selecting previously unselected package fonts-lmodern.\n","Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n","Unpacking fonts-lmodern (2.004.5-6.1) ...\n","Selecting previously unselected package fonts-noto-mono.\n","Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n","Unpacking fonts-noto-mono (20201225-1build1) ...\n","Selecting previously unselected package fonts-texgyre.\n","Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n","Unpacking fonts-texgyre (20180621-3.1) ...\n","Selecting previously unselected package libapache-pom-java.\n","Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n","Unpacking libapache-pom-java (18-1) ...\n","Selecting previously unselected package libcommons-parent-java.\n","Preparing to unpack .../17-libcommons-parent-java_43-1_all.deb ...\n","Unpacking libcommons-parent-java (43-1) ...\n","Selecting previously unselected package libcommons-logging-java.\n","Preparing to unpack .../18-libcommons-logging-java_1.2-2_all.deb ...\n","Unpacking libcommons-logging-java (1.2-2) ...\n","Selecting previously unselected package libfontenc1:amd64.\n","Preparing to unpack .../19-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libptexenc1:amd64.\n","Preparing to unpack .../20-libptexenc1_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package rubygems-integration.\n","Preparing to unpack .../21-rubygems-integration_1.18_all.deb ...\n","Unpacking rubygems-integration (1.18) ...\n","Selecting previously unselected package ruby3.0.\n","Preparing to unpack .../22-ruby3.0_3.0.2-7ubuntu2.4_amd64.deb ...\n","Unpacking ruby3.0 (3.0.2-7ubuntu2.4) ...\n","Selecting previously unselected package ruby-rubygems.\n","Preparing to unpack .../23-ruby-rubygems_3.3.5-2_all.deb ...\n","Unpacking ruby-rubygems (3.3.5-2) ...\n","Selecting previously unselected package ruby.\n","Preparing to unpack .../24-ruby_1%3a3.0~exp1_amd64.deb ...\n","Unpacking ruby (1:3.0~exp1) ...\n","Selecting previously unselected package rake.\n","Preparing to unpack .../25-rake_13.0.6-2_all.deb ...\n","Unpacking rake (13.0.6-2) ...\n","Selecting previously unselected package ruby-net-telnet.\n","Preparing to unpack .../26-ruby-net-telnet_0.1.1-2_all.deb ...\n","Unpacking ruby-net-telnet (0.1.1-2) ...\n","Selecting previously unselected package ruby-webrick.\n","Preparing to unpack .../27-ruby-webrick_1.7.0-3_all.deb ...\n","Unpacking ruby-webrick (1.7.0-3) ...\n","Selecting previously unselected package ruby-xmlrpc.\n","Preparing to unpack .../28-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n","Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n","Selecting previously unselected package libruby3.0:amd64.\n","Preparing to unpack .../29-libruby3.0_3.0.2-7ubuntu2.4_amd64.deb ...\n","Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.4) ...\n","Selecting previously unselected package libsynctex2:amd64.\n","Preparing to unpack .../30-libsynctex2_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libteckit0:amd64.\n","Preparing to unpack .../31-libteckit0_2.5.11+ds1-1_amd64.deb ...\n","Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n","Selecting previously unselected package libtexlua53:amd64.\n","Preparing to unpack .../32-libtexlua53_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libtexluajit2:amd64.\n","Preparing to unpack .../33-libtexluajit2_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libzzip-0-13:amd64.\n","Preparing to unpack .../34-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n","Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../35-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../36-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package lmodern.\n","Preparing to unpack .../37-lmodern_2.004.5-6.1_all.deb ...\n","Unpacking lmodern (2.004.5-6.1) ...\n","Selecting previously unselected package preview-latex-style.\n","Preparing to unpack .../38-preview-latex-style_12.2-1ubuntu1_all.deb ...\n","Unpacking preview-latex-style (12.2-1ubuntu1) ...\n","Selecting previously unselected package t1utils.\n","Preparing to unpack .../39-t1utils_1.41-4build2_amd64.deb ...\n","Unpacking t1utils (1.41-4build2) ...\n","Selecting previously unselected package teckit.\n","Preparing to unpack .../40-teckit_2.5.11+ds1-1_amd64.deb ...\n","Unpacking teckit (2.5.11+ds1-1) ...\n","Selecting previously unselected package tex-gyre.\n","Preparing to unpack .../41-tex-gyre_20180621-3.1_all.deb ...\n","Unpacking tex-gyre (20180621-3.1) ...\n","Selecting previously unselected package texlive-binaries.\n","Preparing to unpack .../42-texlive-binaries_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package texlive-base.\n","Preparing to unpack .../43-texlive-base_2021.20220204-1_all.deb ...\n","Unpacking texlive-base (2021.20220204-1) ...\n","Selecting previously unselected package texlive-fonts-recommended.\n","Preparing to unpack .../44-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n","Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-base.\n","Preparing to unpack .../45-texlive-latex-base_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-base (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-recommended.\n","Preparing to unpack .../46-texlive-latex-recommended_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-recommended (2021.20220204-1) ...\n","Selecting previously unselected package texlive.\n","Preparing to unpack .../47-texlive_2021.20220204-1_all.deb ...\n","Unpacking texlive (2021.20220204-1) ...\n","Selecting previously unselected package libfontbox-java.\n","Preparing to unpack .../48-libfontbox-java_1%3a1.8.16-2_all.deb ...\n","Unpacking libfontbox-java (1:1.8.16-2) ...\n","Selecting previously unselected package libpdfbox-java.\n","Preparing to unpack .../49-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n","Unpacking libpdfbox-java (1:1.8.16-2) ...\n","Selecting previously unselected package texlive-pictures.\n","Preparing to unpack .../50-texlive-pictures_2021.20220204-1_all.deb ...\n","Unpacking texlive-pictures (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-extra.\n","Preparing to unpack .../51-texlive-latex-extra_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-extra (2021.20220204-1) ...\n","Selecting previously unselected package texlive-plain-generic.\n","Preparing to unpack .../52-texlive-plain-generic_2021.20220204-1_all.deb ...\n","Unpacking texlive-plain-generic (2021.20220204-1) ...\n","Selecting previously unselected package tipa.\n","Preparing to unpack .../53-tipa_2%3a1.3-21_all.deb ...\n","Unpacking tipa (2:1.3-21) ...\n","Selecting previously unselected package texlive-xetex.\n","Preparing to unpack .../54-texlive-xetex_2021.20220204-1_all.deb ...\n","Unpacking texlive-xetex (2021.20220204-1) ...\n","Setting up fonts-lato (2.0-2.1) ...\n","Setting up fonts-noto-mono (20201225-1build1) ...\n","Setting up libwoff1:amd64 (1.0.2-1build4) ...\n","Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up libijs-0.35:amd64 (0.35-15build2) ...\n","Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up libfontbox-java (1:1.8.16-2) ...\n","Setting up rubygems-integration (1.18) ...\n","Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n","Setting up fonts-urw-base35 (20200910-1) ...\n","Setting up poppler-data (0.4.11-1) ...\n","Setting up tex-common (6.17) ...\n","update-language: texlive-base not installed and configured, doing nothing!\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n","Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n","Setting up libapache-pom-java (18-1) ...\n","Setting up ruby-net-telnet (0.1.1-2) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up t1utils (1.41-4build2) ...\n","Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n","Setting up fonts-texgyre (20180621-3.1) ...\n","Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up ruby-webrick (1.7.0-3) ...\n","Setting up fonts-lmodern (2.004.5-6.1) ...\n","Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n","Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.5) ...\n","Setting up teckit (2.5.11+ds1-1) ...\n","Setting up libpdfbox-java (1:1.8.16-2) ...\n","Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.5) ...\n","Setting up preview-latex-style (12.2-1ubuntu1) ...\n","Setting up libcommons-parent-java (43-1) ...\n","Setting up dvisvgm (2.13.1-1) ...\n","Setting up libcommons-logging-java (1.2-2) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.1) ...\n","update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n","update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n","Setting up lmodern (2.004.5-6.1) ...\n","Setting up texlive-base (2021.20220204-1) ...\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n","mktexlsr: Updating /var/lib/texmf/ls-R... \n","mktexlsr: Done.\n","tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n","tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n","tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n","tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n","Setting up tex-gyre (20180621-3.1) ...\n","Setting up texlive-plain-generic (2021.20220204-1) ...\n","Setting up texlive-latex-base (2021.20220204-1) ...\n","Setting up texlive-latex-recommended (2021.20220204-1) ...\n","Setting up texlive-pictures (2021.20220204-1) ...\n","Setting up texlive-fonts-recommended (2021.20220204-1) ...\n","Setting up tipa (2:1.3-21) ...\n","Setting up texlive (2021.20220204-1) ...\n","Setting up texlive-latex-extra (2021.20220204-1) ...\n","Setting up texlive-xetex (2021.20220204-1) ...\n","Setting up rake (13.0.6-2) ...\n","Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.4) ...\n","Setting up ruby3.0 (3.0.2-7ubuntu2.4) ...\n","Setting up ruby (1:3.0~exp1) ...\n","Setting up ruby-rubygems (3.3.5-2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","Processing triggers for tex-common (6.17) ...\n","Running updmap-sys. This may take some time... done.\n","Running mktexlsr /var/lib/texmf ... done.\n","Building format(s) --all.\n","\tThis may take some time... done.\n","Collecting pypandoc\n","  Downloading pypandoc-1.12-py3-none-any.whl (20 kB)\n","Installing collected packages: pypandoc\n","Successfully installed pypandoc-1.12\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvAmWXOoWrw7","executionInfo":{"status":"ok","timestamp":1701397028957,"user_tz":360,"elapsed":33957,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"9235a537-c32e-4d4d-9c8f-e40cc5dfa454"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!jupyter nbconvert --to PDF '/content/drive/MyDrive/Colab Notebooks/NLP_HW3_MinjiPark.ipynb'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slJJdYgTWtQA","executionInfo":{"status":"ok","timestamp":1701397044590,"user_tz":360,"elapsed":15636,"user":{"displayName":"Anna !","userId":"17731284774928303452"}},"outputId":"3ab810f4-2954-4e3e-fb41-014a286b5c37"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/NLP_HW3_MinjiPark.ipynb to PDF\n","[NbConvertApp] Writing 118913 bytes to notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 106329 bytes to /content/drive/MyDrive/Colab Notebooks/NLP_HW3_MinjiPark.pdf\n"]}]}]}