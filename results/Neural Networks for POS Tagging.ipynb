{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1**"
      ],
      "metadata": {
        "id": "0Itl9vttF3TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1: A Baseline Neural Netowrk Tagger"
      ],
      "metadata": {
        "id": "EgMvxgWXF7Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xSEAfq7ok0qJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_file(file):\n",
        "    \"\"\"\n",
        "    Parse the TSV file for the POS tagging task.\n",
        "    Args:\n",
        "    file (file object): The file object to be read.\n",
        "    Returns:\n",
        "    list: A list of sentences, where each sentence is represented as a list of (word, tag) tuples.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            word, pos = line.split('\\t')\n",
        "            sentence.append((word, pos))\n",
        "        else:\n",
        "            sentences.append(sentence)\n",
        "            sentence = []\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "rt_bdefEFk8k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "def load_data(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = parse_file(file)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "2NkZsoEYUcNa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = load_data(\"twpos-train.tsv\")\n",
        "dev_sentences = load_data(\"twpos-dev.tsv\")\n",
        "devtest_sentences = load_data(\"twpos-devtest.tsv\")"
      ],
      "metadata": {
        "id": "w5Xv1RAU20Wm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    train_data = train_sentences\n",
        "\n",
        "    word_to_ix = {\"UUUNKKK\": 0, '<s>':1, '</s>':2}\n",
        "    tag_to_ix = {}\n",
        "\n",
        "    # Iterate over each sentence (which is a list of (word, tag) tuples)\n",
        "    for sentence in train_data:\n",
        "        for word, tag in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                # Add the word to the dictionary if it's not already there\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "            if tag not in tag_to_ix:\n",
        "                tag_to_ix[tag] = len(tag_to_ix)\n",
        "    return word_to_ix, tag_to_ix\n",
        "\n",
        "# Usage\n",
        "word_to_ix, tag_to_ix = prepare_data()\n",
        "target_size = len(tag_to_ix)"
      ],
      "metadata": {
        "id": "I1R0iiSZ3DSw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_sentence(data, word_to_ix, tag_to_ix, w):\n",
        "  concat_data = []\n",
        "  tags = []\n",
        "  for sentence in data:\n",
        "      for i in range(len(sentence)):\n",
        "          current = []\n",
        "          for j in range(i - w, i + w + 1):\n",
        "              if j < 0:\n",
        "                word = '</s>'\n",
        "              elif j >= len(sentence):\n",
        "                word = '</s>'\n",
        "              else:\n",
        "                word = sentence[j][0]\n",
        "              if word not in word_to_ix:\n",
        "                word = 'UUUNKKK'\n",
        "              current.append(word_to_ix[word])\n",
        "              if i == j:\n",
        "                tag = tag_to_ix[sentence[j][1]]\n",
        "          concat_data.append(current)\n",
        "          tags.append(tag)\n",
        "  return np.array(concat_data), np.array(tags)"
      ],
      "metadata": {
        "id": "vZvdT2l1Pwfn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger:\n",
        "  def __init__(self, w, vocab_size):\n",
        "    self.w = w\n",
        "    self.dim_e = 50\n",
        "    self.dim_h = 128\n",
        "    self.dim_s = len(tag_to_ix)\n",
        "    self.vocab_size = len(word_to_ix)\n",
        "    self.batch_size = 32\n",
        "    self.epochs = 20"
      ],
      "metadata": {
        "id": "IUMg6Ff0t_vv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, POSTagger):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        #print(embeds.shape)\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, word_to_ix, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "EppCeeOr2o0f"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment demonstrated a clear performance improvement when the context window size was increased from w=0 to w=1, highlighting the model's enhanced ability to leverage additional contextual information for more accurate predictions. This suggests that a larger context window is crucial for capturing necessary context and nuances in language processing tasks."
      ],
      "metadata": {
        "id": "UUnx2FnaqFi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger(w=w, vocab_size = len(word_to_ix))\n",
        "        model = NN(POST)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, word_to_ix, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjhluO6bgYfD",
        "outputId": "ada7e35b-b76c-4e9b-9492-cf43f0016fb1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0, Loss: 2.5900\n",
            "Loss: 2.0071, Accuracy: 34.4534%\n",
            "Epoch 1, Loss: 1.6070\n",
            "Loss: 1.2447, Accuracy: 67.9942%\n",
            "Epoch 2, Loss: 1.0065\n",
            "Loss: 1.0844, Accuracy: 74.0510%\n",
            "Epoch 3, Loss: 0.7084\n",
            "Loss: 1.0854, Accuracy: 75.4200%\n",
            "Epoch 4, Loss: 0.5326\n",
            "Loss: 1.1063, Accuracy: 76.2497%\n",
            "Epoch 5, Loss: 0.4253\n",
            "Loss: 1.1199, Accuracy: 77.5565%\n",
            "Epoch 6, Loss: 0.3792\n",
            "Loss: 1.0919, Accuracy: 77.2869%\n",
            "Epoch 7, Loss: 0.3429\n",
            "Loss: 1.1398, Accuracy: 77.1417%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 1.0484, Accuracy: 78.1850%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0, Loss: 2.4949\n",
            "Loss: 1.6656, Accuracy: 46.5671%\n",
            "Epoch 1, Loss: 1.0897\n",
            "Loss: 0.9314, Accuracy: 73.1176%\n",
            "Epoch 2, Loss: 0.6643\n",
            "Loss: 0.8416, Accuracy: 78.7181%\n",
            "Epoch 3, Loss: 0.4425\n",
            "Loss: 0.8401, Accuracy: 80.2531%\n",
            "Epoch 4, Loss: 0.3368\n",
            "Loss: 0.8810, Accuracy: 79.0500%\n",
            "Epoch 5, Loss: 0.2599\n",
            "Loss: 0.9111, Accuracy: 79.9834%\n",
            "Epoch 6, Loss: 0.2009\n",
            "Loss: 0.9401, Accuracy: 79.7760%\n",
            "Epoch 7, Loss: 0.1599\n",
            "Loss: 0.9838, Accuracy: 80.2738%\n",
            "Epoch 8, Loss: 0.1277\n",
            "Loss: 1.0396, Accuracy: 80.5435%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.9105, Accuracy: 81.2244%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Feature Engineering"
      ],
      "metadata": {
        "id": "Mlmv_c2xGKm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon reviewing misclassified center words from section 1.1, it was observed that inaccuracies often involved words beginning with capital letters, containing special characters, or including numbers. To address this, new features were designed to precisely identify such traits within the words. Incorporating these nuanced features enhanced the model's understanding of the data, leading to a noticeable improvement in accuracy compared to the results in section 1.1."
      ],
      "metadata": {
        "id": "kPTlJmn7rLq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def extract_features(word):\n",
        "    features = [\n",
        "        float(any(char.isdigit() for char in word)),  # contains digits\n",
        "        float(word.isdigit()),\n",
        "        float(any(char in string.punctuation for char in word)),  # contains punctuation\n",
        "        word[0] in string.punctuation,\n",
        "        len(word)\n",
        "    ]\n",
        "    return features"
      ],
      "metadata": {
        "id": "jGZoxxyh6ME0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_sentence_features(sentences, w):\n",
        "  feature_matrix = []\n",
        "  for sentence in sentences:\n",
        "      for i in range(len(sentence)):\n",
        "          for j in range(i - w, i + w + 1):\n",
        "                if i == j:\n",
        "                  features = extract_features(sentence[j][0])\n",
        "                  feature_matrix.append(features)\n",
        "  return np.array(feature_matrix)"
      ],
      "metadata": {
        "id": "9UZQmk73064c"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_feature(nn.Module):\n",
        "    def __init__(self, POSTagger):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.num_features = 5\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e  + self.num_features, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs, features = inputs\n",
        "        embeds = self.embeddings(inputs).view(-1, ((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e))\n",
        "        combined = torch.cat((embeds, features), 1)\n",
        "        hidden_out = self.hidden(combined)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, word_to_ix, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.train_features = transfer_sentence_features(train_sentence, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_features = transfer_sentence_features(dev_sentence, self.POSTagger.w)\n",
        "\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_features = transfer_sentence_features(devtest_sentence, self.POSTagger.w)\n",
        "\n",
        "        # Convert the feature data from NumPy arrays to tensors\n",
        "        self.train_features = torch.tensor(self.train_features, dtype=torch.float)\n",
        "        self.dev_features = torch.tensor(self.dev_features, dtype=torch.float)\n",
        "        self.devtest_features = torch.tensor(self.devtest_features, dtype=torch.float)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label, features):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "        feature_s = features[idx]\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            feature_batch = feature_s[i:i + self.POSTagger.batch_size].clone().detach()\n",
        "\n",
        "            loss = self.run_grad((x, feature_batch), y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label, features):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            feature_batch = features.clone().detach()\n",
        "            loss = self.get_loss((x, feature_batch), y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward((x,feature_batch))\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "        test_loss /= n\n",
        "        accuracy = (correct / n) * 100\n",
        "        print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "        return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label, self.train_features)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label, self.dev_features)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label, self.devtest_features)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "2bl71q6nhHSv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger(w=w, vocab_size = len(word_to_ix))\n",
        "        model = NN_feature(POST)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, word_to_ix, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0fDjYdvppi8",
        "outputId": "e4f38bcf-0efa-4bb2-9207-32d5c58fa1c9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0, Loss: 1.9213\n",
            "Loss: 1.3358, Accuracy: 56.5235%\n",
            "Epoch 1, Loss: 1.1825\n",
            "Loss: 1.1400, Accuracy: 66.8326%\n",
            "Epoch 2, Loss: 0.9413\n",
            "Loss: 0.9109, Accuracy: 73.7399%\n",
            "Epoch 3, Loss: 0.7751\n",
            "Loss: 1.0900, Accuracy: 73.4287%\n",
            "Epoch 4, Loss: 0.6167\n",
            "Loss: 0.9947, Accuracy: 75.2126%\n",
            "Epoch 5, Loss: 0.5438\n",
            "Loss: 1.0348, Accuracy: 75.5445%\n",
            "Epoch 6, Loss: 0.4647\n",
            "Loss: 1.0432, Accuracy: 77.5150%\n",
            "Epoch 7, Loss: 0.4046\n",
            "Loss: 1.0194, Accuracy: 77.2039%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.9623, Accuracy: 78.6808%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0, Loss: 1.9358\n",
            "Loss: 1.3448, Accuracy: 57.7681%\n",
            "Epoch 1, Loss: 1.1161\n",
            "Loss: 0.9713, Accuracy: 68.9691%\n",
            "Epoch 2, Loss: 0.8139\n",
            "Loss: 0.7943, Accuracy: 75.2748%\n",
            "Epoch 3, Loss: 0.6369\n",
            "Loss: 0.7428, Accuracy: 79.5271%\n",
            "Epoch 4, Loss: 0.5153\n",
            "Loss: 0.7614, Accuracy: 78.2410%\n",
            "Epoch 5, Loss: 0.3979\n",
            "Loss: 0.7333, Accuracy: 80.3568%\n",
            "Epoch 6, Loss: 0.3226\n",
            "Loss: 0.6962, Accuracy: 81.6013%\n",
            "Epoch 7, Loss: 0.2797\n",
            "Loss: 0.7801, Accuracy: 81.3109%\n",
            "Epoch 8, Loss: 0.2217\n",
            "Loss: 0.8112, Accuracy: 81.7050%\n",
            "Epoch 9, Loss: 0.1919\n",
            "Loss: 0.8480, Accuracy: 81.3732%\n",
            "Epoch 10, Loss: 0.1629\n",
            "Loss: 0.8641, Accuracy: 81.2902%\n",
            "Epoch 11, Loss: 0.1437\n",
            "Loss: 0.9294, Accuracy: 80.1286%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.8153, Accuracy: 81.8280%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Pretrained Embeddings"
      ],
      "metadata": {
        "id": "BdD3oXi4uCLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Experiment with updating the pretrained embeddings for w = 0 and w = 1"
      ],
      "metadata": {
        "id": "krSXD6h8UUsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Setting up the train an embedding for '&lt;s&gt;' by using the embedding for '&lt;s&gt;'"
      ],
      "metadata": {
        "id": "gggKl_PQsFaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def load_embeddings(embedding_file):\n",
        "    embeddings = {}\n",
        "    with open(embedding_file, 'r', encoding='utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        words = line.split()\n",
        "        embeddings[words[0]] = words[1:]\n",
        "        if words[0] == '<\\s>':\n",
        "          embeddings['<s>'] = words[1:]\n",
        "    return pd.DataFrame.from_dict(embeddings, orient = 'index').astype('float32')"
      ],
      "metadata": {
        "id": "jqJwin-oFOkh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_df = load_embeddings('/content/twitter-embeddings.txt')"
      ],
      "metadata": {
        "id": "BAEjTgjRJIz_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_to_index = {word: i for i, word in enumerate(embedding_df.index)}"
      ],
      "metadata": {
        "id": "M-yIPcuEL0T0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger_embed:\n",
        "  def __init__(self, w, vocab_size):\n",
        "    self.w = w\n",
        "    self.dim_e = 50\n",
        "    self.dim_h = 128\n",
        "    self.dim_s = len(tag_to_ix)\n",
        "    self.vocab_size = len(twitter_to_index)\n",
        "    self.batch_size = 32\n",
        "    self.epochs = 20"
      ],
      "metadata": {
        "id": "KOAMqHxSP44f"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_embed(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        #print(embeds.shape)\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "N3XALycWE8fn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment demonstrated accuracy improvements for both w=0 and w=1 settings when using pre-trained embeddings, as opposed to randomly-initialized word embeddings. This underscores the superiority of adopting pre-trained embeddings for enhanced model performance."
      ],
      "metadata": {
        "id": "lFHuBDyqtnh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger_embed(w=w, vocab_size = len(embedding_df))\n",
        "        model = NN_embed(POST, pretrained_embeddings, twitter_to_index)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq7LT2znGxXF",
        "outputId": "caab2a29-bb27-4100-9756-9b5c73d4bed3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0320\n",
            "Loss: 0.6079, Accuracy: 83.4267%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5181\n",
            "Loss: 0.6042, Accuracy: 82.9496%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4424\n",
            "Loss: 0.5458, Accuracy: 83.4474%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4035\n",
            "Loss: 0.5605, Accuracy: 82.4310%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3849\n",
            "Loss: 0.5669, Accuracy: 83.8000%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3709\n",
            "Loss: 0.5730, Accuracy: 83.1363%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3612\n",
            "Loss: 0.5700, Accuracy: 82.6799%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3563\n",
            "Loss: 0.5835, Accuracy: 82.9081%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5359, Accuracy: 83.3369%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.5975\n",
            "Loss: 0.5199, Accuracy: 85.4802%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.2835\n",
            "Loss: 0.5120, Accuracy: 86.2477%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2425\n",
            "Loss: 0.5079, Accuracy: 86.9322%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2071\n",
            "Loss: 0.5447, Accuracy: 86.5588%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1786\n",
            "Loss: 0.5630, Accuracy: 86.2477%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1565\n",
            "Loss: 0.5858, Accuracy: 86.1025%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1337\n",
            "Loss: 0.6198, Accuracy: 85.8743%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1171\n",
            "Loss: 0.6512, Accuracy: 85.9988%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5891, Accuracy: 86.9153%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Comparing the result for updating the pretrained word embeddings during training and keeping them fixed"
      ],
      "metadata": {
        "id": "1Z2Zg3VCUWnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_embed_freeze(nn.Module):\n",
        "    def __init__(self, POSTagger, pretrained_embeddings, twitter_to_index, freeze_embeddings):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.freeze_embeddings = freeze_embeddings\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.parameters()), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.embeddings.weight.requires_grad = not self.freeze_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "\n",
        "            print('------------------------------------------')\n",
        "\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "HcjNJmMCUbw_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training with a context window of w=1, updating pre-trained word embeddings has shown distinct benefits, evidenced by a modest yet noteworthy improvement of approximately 2% in accuracy compared to keeping them fixed. This enhancement underscores the effectiveness of allowing the embeddings to evolve with training."
      ],
      "metadata": {
        "id": "lmnhTRiYvrO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    freeze = [True, False]\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    for f in freeze:\n",
        "        print(f\"\\nParameter Freeze = {f}\")\n",
        "        POST = POSTagger_embed(w = 1, vocab_size = len(embedding_df))\n",
        "        model = NN_embed_freeze(POST, pretrained_embeddings, twitter_to_index, f)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "id": "VViBnjdYlG51",
        "outputId": "0d815a88-6e88-46b4-9fbc-63c6b210e56a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameter Freeze = True\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0161\n",
            "Loss: 0.6291, Accuracy: 82.4310%\n",
            "------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5568\n",
            "Loss: 0.5863, Accuracy: 83.6963%\n",
            "------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5014\n",
            "Loss: 0.5332, Accuracy: 85.2935%\n",
            "------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4665\n",
            "Loss: 0.5152, Accuracy: 85.2935%\n",
            "------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4430\n",
            "Loss: 0.5113, Accuracy: 85.4594%\n",
            "------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.4219\n",
            "Loss: 0.5103, Accuracy: 85.8121%\n",
            "------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.4047\n",
            "Loss: 0.5055, Accuracy: 85.5424%\n",
            "------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3877\n",
            "Loss: 0.5037, Accuracy: 86.0195%\n",
            "------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3727\n",
            "Loss: 0.4963, Accuracy: 86.3099%\n",
            "------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3565\n",
            "Loss: 0.4981, Accuracy: 86.4136%\n",
            "------------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3403\n",
            "Loss: 0.5025, Accuracy: 86.2269%\n",
            "------------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3272\n",
            "Loss: 0.4995, Accuracy: 86.4758%\n",
            "------------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.3146\n",
            "Loss: 0.5320, Accuracy: 85.4387%\n",
            "------------------------------------------\n",
            "Epoch 13\n",
            "Epoch 13, Loss: 0.3022\n",
            "Loss: 0.5020, Accuracy: 86.4966%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.4879, Accuracy: 86.5488%\n",
            "\n",
            "Parameter Freeze = False\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9220\n",
            "Loss: 0.5385, Accuracy: 84.6505%\n",
            "------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4074\n",
            "Loss: 0.4947, Accuracy: 86.0402%\n",
            "------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3033\n",
            "Loss: 0.4777, Accuracy: 86.7247%\n",
            "------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2453\n",
            "Loss: 0.5056, Accuracy: 87.1189%\n",
            "------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2010\n",
            "Loss: 0.5386, Accuracy: 86.4136%\n",
            "------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1731\n",
            "Loss: 0.5659, Accuracy: 86.6003%\n",
            "------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1476\n",
            "Loss: 0.5932, Accuracy: 86.0817%\n",
            "------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1241\n",
            "Loss: 0.6069, Accuracy: 86.5173%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5631, Accuracy: 87.1093%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Feature Engineering for pretrained embeddings model"
      ],
      "metadata": {
        "id": "pnvbzmOoYRmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_embed_feature(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.num_features = 5\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e  + self.num_features, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "    def forward(self, inputs):\n",
        "        inputs, features = inputs\n",
        "        embeds = self.embeddings(inputs).view(-1, ((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e))\n",
        "        combined = torch.cat((embeds, features), 1)\n",
        "        hidden_out = self.hidden(combined)\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, word_to_ix, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.train_features = transfer_sentence_features(train_sentence, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_features = transfer_sentence_features(dev_sentence, self.POSTagger.w)\n",
        "\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, word_to_ix, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_features = transfer_sentence_features(devtest_sentence, self.POSTagger.w)\n",
        "\n",
        "        # Convert the feature data from NumPy arrays to tensors\n",
        "        self.train_features = torch.tensor(self.train_features, dtype=torch.float)\n",
        "        self.dev_features = torch.tensor(self.dev_features, dtype=torch.float)\n",
        "        self.devtest_features = torch.tensor(self.devtest_features, dtype=torch.float)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label, features):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "        feature_s = features[idx]\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            feature_batch = feature_s[i:i + self.POSTagger.batch_size].clone().detach()\n",
        "            loss = self.run_grad((x, feature_batch), y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label, features):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            feature_batch = features.clone().detach()\n",
        "            loss = self.get_loss((x, feature_batch), y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward((x,feature_batch))\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "        test_loss /= n\n",
        "        accuracy = (correct / n) * 100\n",
        "        print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "        return test_loss\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label, self.train_features)\n",
        "            dev_loss = self.test(self.dev_data, self.dev_label, self.dev_features)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss = self.test(self.devtest_data, self.devtest_label, self.devtest_features)\n",
        "        return train_loss, dev_loss, devtest_loss"
      ],
      "metadata": {
        "id": "c4Iio5OiYcGl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining the custom features from Section 1.2 with pre-trained embeddings reveals that these features still offer assistance. Although the improvement is subtle, it highlights that features can contribute additional context that pretraining might not capture, thereby providing a slight edge in the model's performance."
      ],
      "metadata": {
        "id": "7ua-serjxtKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    window_sizes = [0, 1]\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    for w in window_sizes:\n",
        "        print(f\"\\nTraining with context window size w={w}\")\n",
        "        POST = POSTagger_embed(w=w, vocab_size = len(embedding_df))\n",
        "        model = NN_embed_feature(POST, pretrained_embeddings, twitter_to_index)\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "        model.fit()"
      ],
      "metadata": {
        "id": "qYKBx8DoznpA",
        "outputId": "83e72c8a-bbff-446b-c905-224d5ecd46cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.2374\n",
            "Loss: 0.6162, Accuracy: 81.2072%\n",
            "------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.6115\n",
            "Loss: 0.5858, Accuracy: 81.8917%\n",
            "------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5042\n",
            "Loss: 0.5449, Accuracy: 84.0075%\n",
            "------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4308\n",
            "Loss: 0.6345, Accuracy: 82.8044%\n",
            "------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3902\n",
            "Loss: 0.5461, Accuracy: 83.6963%\n",
            "------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3773\n",
            "Loss: 0.5465, Accuracy: 83.9867%\n",
            "------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3592\n",
            "Loss: 0.5755, Accuracy: 84.4638%\n",
            "------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3490\n",
            "Loss: 0.5793, Accuracy: 83.5719%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5350, Accuracy: 83.7465%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.7493\n",
            "Loss: 0.5819, Accuracy: 85.1691%\n",
            "------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3173\n",
            "Loss: 0.5568, Accuracy: 85.2313%\n",
            "------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2650\n",
            "Loss: 0.5588, Accuracy: 84.7957%\n",
            "------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2248\n",
            "Loss: 0.6014, Accuracy: 85.5424%\n",
            "------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2105\n",
            "Loss: 0.5569, Accuracy: 86.7247%\n",
            "------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1832\n",
            "Loss: 0.5502, Accuracy: 87.7619%\n",
            "------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1605\n",
            "Loss: 0.5557, Accuracy: 86.8492%\n",
            "------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1469\n",
            "Loss: 0.6136, Accuracy: 86.5795%\n",
            "------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.1285\n",
            "Loss: 0.6433, Accuracy: 86.4758%\n",
            "------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.1187\n",
            "Loss: 0.6492, Accuracy: 87.0774%\n",
            "------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.1034\n",
            "Loss: 0.6540, Accuracy: 87.5544%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.6157, Accuracy: 87.6051%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Architecture Engineering"
      ],
      "metadata": {
        "id": "XdfzMAZgnCs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I got the best test accuracy when window size = 1 and no hidden layer with tanh function.**"
      ],
      "metadata": {
        "id": "itGBu_pfCCe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Compare the use of 0, 1, and 2 hidden layers"
      ],
      "metadata": {
        "id": "Q99h1r2mnGGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_layers(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index, hidden_dim1=None, hidden_dim2=None):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "        self.hidden1 = None\n",
        "        self.hidden2 = None\n",
        "\n",
        "        input_dim = (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e\n",
        "\n",
        "        if hidden_dim1:\n",
        "            self.hidden1 = nn.Linear(input_dim, hidden_dim1)\n",
        "            output_dim = hidden_dim1\n",
        "\n",
        "            if hidden_dim2:\n",
        "                self.hidden2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "                output_dim = hidden_dim2\n",
        "        else:\n",
        "            output_dim = input_dim\n",
        "        self.output = nn.Linear(output_dim, self.POSTagger.dim_s)\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        if self.hidden1 is not None:\n",
        "          self.hidden1.weight.data.uniform_(-initrange, initrange)\n",
        "        if self.hidden2 is not None:\n",
        "          self.hidden2.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        if self.hidden1 is not None:\n",
        "          hidden_out = self.hidden1(embeds)\n",
        "          if self.hidden2 is not None:\n",
        "            hidden_out = self.hidden2(hidden_out)\n",
        "        else:\n",
        "          hidden_out = embeds\n",
        "        hidden_activated = torch.tanh(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('---------------------------------------------')\n",
        "        print('\\n Testing Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "E4NsmWgFnEyV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [\n",
        "    # 0 hidden layers\n",
        "   {'hidden_layers': 0, 'hidden_dim1': None, 'hidden_dim2': None},\n",
        "\n",
        "    # 1 hidden layer, two setups\n",
        "    {'hidden_layers': 1, 'hidden_dim1': 256, 'hidden_dim2': None},\n",
        "    {'hidden_layers': 1, 'hidden_dim1': 512, 'hidden_dim2': None},\n",
        "\n",
        "    # 2 hidden layers, two setups\n",
        "    {'hidden_layers': 2, 'hidden_dim1': 256, 'hidden_dim2': 128},\n",
        "    {'hidden_layers': 2, 'hidden_dim1': 512, 'hidden_dim2': 256}\n",
        "]\n",
        "\n",
        "# Adding window sizes to test\n",
        "window_sizes = [0, 1]\n",
        "\n",
        "# This will be used to collect all results\n",
        "all_results = []\n",
        "\n",
        "for window in window_sizes:\n",
        "    results = []  # This collects results for the current window size\n",
        "\n",
        "    for config in configs:\n",
        "        print(f\"Training configuration: {config}, Window size: {window}\")\n",
        "\n",
        "        # Initialize the model with the current configuration\n",
        "        pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "        POST = POSTagger_embed(w=window, vocab_size=len(embedding_df))\n",
        "        model = NN_layers(POST, pretrained_embeddings, twitter_to_index,\n",
        "                          hidden_dim1=config['hidden_dim1'],\n",
        "                          hidden_dim2=config['hidden_dim2'])\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "\n",
        "        result = model.fit()\n",
        "\n",
        "        if isinstance(result, tuple):\n",
        "            result = {\n",
        "                'train_loss': result[0],\n",
        "                'dev_loss': result[1],\n",
        "                'devtest_loss': result[2],\n",
        "                'dev_accuracy': result[3],\n",
        "                'devtest_accuracy': result[4]\n",
        "            }\n",
        "\n",
        "        result['window_size'] = window\n",
        "        result.update(config)\n",
        "        results.append(result)\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(f\"Results for window size {window}:\\n\", results_df)\n",
        "    all_results.extend(results)\n",
        "all_results_df = pd.DataFrame(all_results)\n",
        "print(\"Complete results:\\n\", all_results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPHzz6n8xkfH",
        "outputId": "8df1e91d-ce47-44d2-b9ae-52c874e18f97"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration: {'hidden_layers': 0, 'hidden_dim1': None, 'hidden_dim2': None}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9941\n",
            "Loss: 0.6275, Accuracy: 81.2280%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5282\n",
            "Loss: 0.5521, Accuracy: 83.1363%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4452\n",
            "Loss: 0.5322, Accuracy: 84.3186%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4007\n",
            "Loss: 0.5265, Accuracy: 84.2771%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3729\n",
            "Loss: 0.5343, Accuracy: 83.1155%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3577\n",
            "Loss: 0.5497, Accuracy: 83.8623%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3465\n",
            "Loss: 0.5443, Accuracy: 83.8623%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3387\n",
            "Loss: 0.5386, Accuracy: 84.3601%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3342\n",
            "Loss: 0.5516, Accuracy: 83.0741%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5144, Accuracy: 83.7034%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 256, 'hidden_dim2': None}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9895\n",
            "Loss: 0.6243, Accuracy: 82.8666%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5077\n",
            "Loss: 0.5451, Accuracy: 84.0697%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4443\n",
            "Loss: 0.5602, Accuracy: 83.5304%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4089\n",
            "Loss: 0.5446, Accuracy: 83.5096%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3882\n",
            "Loss: 0.5655, Accuracy: 82.9911%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3747\n",
            "Loss: 0.5807, Accuracy: 82.5970%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3691\n",
            "Loss: 0.5687, Accuracy: 83.3852%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3660\n",
            "Loss: 0.5495, Accuracy: 84.0075%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3569\n",
            "Loss: 0.5734, Accuracy: 84.2979%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5313, Accuracy: 84.4794%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 512, 'hidden_dim2': None}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9570\n",
            "Loss: 0.5731, Accuracy: 84.3601%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5044\n",
            "Loss: 0.5762, Accuracy: 84.0697%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4371\n",
            "Loss: 0.5784, Accuracy: 81.9540%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4046\n",
            "Loss: 0.5612, Accuracy: 83.5719%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3833\n",
            "Loss: 0.5947, Accuracy: 81.9954%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3768\n",
            "Loss: 0.5531, Accuracy: 84.1734%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3679\n",
            "Loss: 0.5683, Accuracy: 82.6385%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3615\n",
            "Loss: 0.5532, Accuracy: 83.1985%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3576\n",
            "Loss: 0.5440, Accuracy: 83.9867%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3525\n",
            "Loss: 0.5529, Accuracy: 84.2356%\n",
            "---------------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3480\n",
            "Loss: 0.5672, Accuracy: 83.5511%\n",
            "---------------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3461\n",
            "Loss: 0.5703, Accuracy: 82.6799%\n",
            "---------------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.3462\n",
            "Loss: 0.5589, Accuracy: 83.8208%\n",
            "---------------------------------------------\n",
            "Epoch 13\n",
            "Epoch 13, Loss: 0.3422\n",
            "Loss: 0.5826, Accuracy: 84.0282%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5435, Accuracy: 84.1776%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 256, 'hidden_dim2': 128}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.5305\n",
            "Loss: 0.8323, Accuracy: 77.0587%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.7080\n",
            "Loss: 0.6871, Accuracy: 81.9747%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5212\n",
            "Loss: 0.6390, Accuracy: 82.4725%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4462\n",
            "Loss: 0.6842, Accuracy: 81.7673%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4040\n",
            "Loss: 0.6584, Accuracy: 81.9125%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3863\n",
            "Loss: 0.6386, Accuracy: 82.2029%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3686\n",
            "Loss: 0.6228, Accuracy: 83.6341%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3571\n",
            "Loss: 0.6499, Accuracy: 82.9289%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3466\n",
            "Loss: 0.6888, Accuracy: 81.9747%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3429\n",
            "Loss: 0.6668, Accuracy: 82.5140%\n",
            "---------------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3355\n",
            "Loss: 0.6994, Accuracy: 82.4103%\n",
            "---------------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3325\n",
            "Loss: 0.6893, Accuracy: 83.3022%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.6158, Accuracy: 83.6172%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 512, 'hidden_dim2': 256}, Window size: 0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.4180\n",
            "Loss: 0.7776, Accuracy: 79.2159%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.6531\n",
            "Loss: 0.6458, Accuracy: 82.5347%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5276\n",
            "Loss: 0.6439, Accuracy: 81.6013%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4671\n",
            "Loss: 0.6148, Accuracy: 82.7007%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4267\n",
            "Loss: 0.6808, Accuracy: 81.7880%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.4067\n",
            "Loss: 0.6224, Accuracy: 82.8666%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3891\n",
            "Loss: 0.6063, Accuracy: 82.6177%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3768\n",
            "Loss: 0.6894, Accuracy: 80.9376%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3672\n",
            "Loss: 0.6165, Accuracy: 82.5140%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3574\n",
            "Loss: 0.6106, Accuracy: 83.7171%\n",
            "---------------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3467\n",
            "Loss: 0.6266, Accuracy: 83.1570%\n",
            "---------------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3399\n",
            "Loss: 0.6664, Accuracy: 82.4725%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.6150, Accuracy: 82.3669%\n",
            "Results for window size 0:\n",
            "    train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.334195  0.551624      0.514400     83.074051         83.703384   \n",
            "1    0.356924  0.573442      0.531290     84.297864         84.479414   \n",
            "2    0.342210  0.582594      0.543496     84.028210         84.177624   \n",
            "3    0.332495  0.689333      0.615828     83.302219         83.617159   \n",
            "4    0.339902  0.666445      0.614963     82.472516         82.366889   \n",
            "\n",
            "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
            "0            0              0          NaN          NaN  \n",
            "1            0              1        256.0          NaN  \n",
            "2            0              1        512.0          NaN  \n",
            "3            0              2        256.0        128.0  \n",
            "4            0              2        512.0        256.0  \n",
            "Training configuration: {'hidden_layers': 0, 'hidden_dim1': None, 'hidden_dim2': None}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.8745\n",
            "Loss: 0.5300, Accuracy: 85.9158%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4232\n",
            "Loss: 0.4530, Accuracy: 87.2641%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3308\n",
            "Loss: 0.4311, Accuracy: 87.8863%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2719\n",
            "Loss: 0.4410, Accuracy: 87.4300%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2322\n",
            "Loss: 0.4307, Accuracy: 87.5752%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2034\n",
            "Loss: 0.4351, Accuracy: 87.9693%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1824\n",
            "Loss: 0.4820, Accuracy: 87.0774%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1649\n",
            "Loss: 0.4492, Accuracy: 87.8034%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.1487\n",
            "Loss: 0.4570, Accuracy: 87.7204%\n",
            "---------------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.1339\n",
            "Loss: 0.4694, Accuracy: 87.4507%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.4198, Accuracy: 88.4458%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 256, 'hidden_dim2': None}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.8778\n",
            "Loss: 0.5356, Accuracy: 85.1068%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4023\n",
            "Loss: 0.4888, Accuracy: 86.3514%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3023\n",
            "Loss: 0.5118, Accuracy: 85.8121%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2433\n",
            "Loss: 0.5409, Accuracy: 85.8328%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2033\n",
            "Loss: 0.5385, Accuracy: 86.2477%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1751\n",
            "Loss: 0.5652, Accuracy: 86.6003%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1515\n",
            "Loss: 0.6252, Accuracy: 86.3721%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5681, Accuracy: 87.1524%\n",
            "Training configuration: {'hidden_layers': 1, 'hidden_dim1': 512, 'hidden_dim2': None}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.8543\n",
            "Loss: 0.4902, Accuracy: 85.9988%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3889\n",
            "Loss: 0.4607, Accuracy: 87.2226%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2977\n",
            "Loss: 0.4984, Accuracy: 86.4758%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2436\n",
            "Loss: 0.5057, Accuracy: 86.9737%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2034\n",
            "Loss: 0.5125, Accuracy: 87.3885%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1673\n",
            "Loss: 0.5561, Accuracy: 86.5795%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1482\n",
            "Loss: 0.6290, Accuracy: 86.1025%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.5535, Accuracy: 87.1524%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 256, 'hidden_dim2': 128}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.4576\n",
            "Loss: 0.8771, Accuracy: 75.9386%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.6015\n",
            "Loss: 0.6050, Accuracy: 83.1155%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4319\n",
            "Loss: 0.5488, Accuracy: 84.8994%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3387\n",
            "Loss: 0.5467, Accuracy: 85.7498%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2868\n",
            "Loss: 0.5952, Accuracy: 84.9409%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2521\n",
            "Loss: 0.6224, Accuracy: 85.2728%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.2198\n",
            "Loss: 0.6301, Accuracy: 85.2520%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1931\n",
            "Loss: 0.7209, Accuracy: 83.9038%\n",
            "---------------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.1728\n",
            "Loss: 0.6785, Accuracy: 85.2313%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.6384, Accuracy: 86.1824%\n",
            "Training configuration: {'hidden_layers': 2, 'hidden_dim1': 512, 'hidden_dim2': 256}, Window size: 1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.3084\n",
            "Loss: 0.6748, Accuracy: 81.3109%\n",
            "---------------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5433\n",
            "Loss: 0.6368, Accuracy: 82.6592%\n",
            "---------------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4152\n",
            "Loss: 0.5308, Accuracy: 84.9824%\n",
            "---------------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3424\n",
            "Loss: 0.5542, Accuracy: 85.2935%\n",
            "---------------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2830\n",
            "Loss: 0.6122, Accuracy: 84.4845%\n",
            "---------------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2435\n",
            "Loss: 0.6118, Accuracy: 84.2771%\n",
            "---------------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.2183\n",
            "Loss: 0.6487, Accuracy: 84.7542%\n",
            "---------------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1988\n",
            "Loss: 0.7016, Accuracy: 84.7957%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            " Testing Result\n",
            "Loss: 0.6507, Accuracy: 86.0961%\n",
            "Results for window size 1:\n",
            "    train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.133885  0.469412      0.419780     87.450736         88.445786   \n",
            "1    0.151463  0.625214      0.568134     86.372122         87.152404   \n",
            "2    0.148190  0.628975      0.553454     86.102468         87.152404   \n",
            "3    0.172792  0.678500      0.638390     85.231280         86.182367   \n",
            "4    0.198769  0.701571      0.650674     84.795686         86.096141   \n",
            "\n",
            "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
            "0            1              0          NaN          NaN  \n",
            "1            1              1        256.0          NaN  \n",
            "2            1              1        512.0          NaN  \n",
            "3            1              2        256.0        128.0  \n",
            "4            1              2        512.0        256.0  \n",
            "Complete results:\n",
            "    train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.334195  0.551624      0.514400     83.074051         83.703384   \n",
            "1    0.356924  0.573442      0.531290     84.297864         84.479414   \n",
            "2    0.342210  0.582594      0.543496     84.028210         84.177624   \n",
            "3    0.332495  0.689333      0.615828     83.302219         83.617159   \n",
            "4    0.339902  0.666445      0.614963     82.472516         82.366889   \n",
            "5    0.133885  0.469412      0.419780     87.450736         88.445786   \n",
            "6    0.151463  0.625214      0.568134     86.372122         87.152404   \n",
            "7    0.148190  0.628975      0.553454     86.102468         87.152404   \n",
            "8    0.172792  0.678500      0.638390     85.231280         86.182367   \n",
            "9    0.198769  0.701571      0.650674     84.795686         86.096141   \n",
            "\n",
            "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
            "0            0              0          NaN          NaN  \n",
            "1            0              1        256.0          NaN  \n",
            "2            0              1        512.0          NaN  \n",
            "3            0              2        256.0        128.0  \n",
            "4            0              2        512.0        256.0  \n",
            "5            1              0          NaN          NaN  \n",
            "6            1              1        256.0          NaN  \n",
            "7            1              1        512.0          NaN  \n",
            "8            1              2        256.0        128.0  \n",
            "9            1              2        512.0        256.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both for the window sizes 0 and 1, the test accuracy decreased as the model increased the number of layers and neurons in the layers."
      ],
      "metadata": {
        "id": "Mm3fW9bQCu1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results_df"
      ],
      "metadata": {
        "id": "0Lzbn_HLCbiC",
        "outputId": "d6f1ccec-e0d8-4d87-9a0e-d048ec05a7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.334195  0.551624      0.514400     83.074051         83.703384   \n",
              "1    0.356924  0.573442      0.531290     84.297864         84.479414   \n",
              "2    0.342210  0.582594      0.543496     84.028210         84.177624   \n",
              "3    0.332495  0.689333      0.615828     83.302219         83.617159   \n",
              "4    0.339902  0.666445      0.614963     82.472516         82.366889   \n",
              "5    0.133885  0.469412      0.419780     87.450736         88.445786   \n",
              "6    0.151463  0.625214      0.568134     86.372122         87.152404   \n",
              "7    0.148190  0.628975      0.553454     86.102468         87.152404   \n",
              "8    0.172792  0.678500      0.638390     85.231280         86.182367   \n",
              "9    0.198769  0.701571      0.650674     84.795686         86.096141   \n",
              "\n",
              "   window_size  hidden_layers  hidden_dim1  hidden_dim2  \n",
              "0            0              0          NaN          NaN  \n",
              "1            0              1        256.0          NaN  \n",
              "2            0              1        512.0          NaN  \n",
              "3            0              2        256.0        128.0  \n",
              "4            0              2        512.0        256.0  \n",
              "5            1              0          NaN          NaN  \n",
              "6            1              1        256.0          NaN  \n",
              "7            1              1        512.0          NaN  \n",
              "8            1              2        256.0        128.0  \n",
              "9            1              2        512.0        256.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-985db51f-98b6-416c-9f91-7ce0ba8e0d00\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>hidden_dim1</th>\n",
              "      <th>hidden_dim2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.334195</td>\n",
              "      <td>0.551624</td>\n",
              "      <td>0.514400</td>\n",
              "      <td>83.074051</td>\n",
              "      <td>83.703384</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.356924</td>\n",
              "      <td>0.573442</td>\n",
              "      <td>0.531290</td>\n",
              "      <td>84.297864</td>\n",
              "      <td>84.479414</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>256.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.342210</td>\n",
              "      <td>0.582594</td>\n",
              "      <td>0.543496</td>\n",
              "      <td>84.028210</td>\n",
              "      <td>84.177624</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>512.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.332495</td>\n",
              "      <td>0.689333</td>\n",
              "      <td>0.615828</td>\n",
              "      <td>83.302219</td>\n",
              "      <td>83.617159</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.339902</td>\n",
              "      <td>0.666445</td>\n",
              "      <td>0.614963</td>\n",
              "      <td>82.472516</td>\n",
              "      <td>82.366889</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.133885</td>\n",
              "      <td>0.469412</td>\n",
              "      <td>0.419780</td>\n",
              "      <td>87.450736</td>\n",
              "      <td>88.445786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.151463</td>\n",
              "      <td>0.625214</td>\n",
              "      <td>0.568134</td>\n",
              "      <td>86.372122</td>\n",
              "      <td>87.152404</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>256.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.148190</td>\n",
              "      <td>0.628975</td>\n",
              "      <td>0.553454</td>\n",
              "      <td>86.102468</td>\n",
              "      <td>87.152404</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>512.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.172792</td>\n",
              "      <td>0.678500</td>\n",
              "      <td>0.638390</td>\n",
              "      <td>85.231280</td>\n",
              "      <td>86.182367</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.198769</td>\n",
              "      <td>0.701571</td>\n",
              "      <td>0.650674</td>\n",
              "      <td>84.795686</td>\n",
              "      <td>86.096141</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-985db51f-98b6-416c-9f91-7ce0ba8e0d00')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-985db51f-98b6-416c-9f91-7ce0ba8e0d00 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-985db51f-98b6-416c-9f91-7ce0ba8e0d00');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8c1ee4e2-1660-4d3b-9a82-d03b2b5d2431\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c1ee4e2-1660-4d3b-9a82-d03b2b5d2431')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8c1ee4e2-1660-4d3b-9a82-d03b2b5d2431 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Experiment with different nonlinearities(Identity, tanh, ReLU, Sigmoid) with number of hidden layers = 1, hidden layer of width = 128"
      ],
      "metadata": {
        "id": "gbz8wz6Tljva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_activation(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index, activation_function):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h, self.POSTagger.dim_s)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation_function == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation_function == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation_function == 'identity':\n",
        "            self.activation = lambda x: x\n",
        "        else:\n",
        "            self.activation = nn.Tanh()  # Default is tanh\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings):\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "        self.hidden.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        hidden_out = self.hidden(embeds)\n",
        "        hidden_activated = self.activation(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "8orv1lGejuOq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = []\n",
        "\n",
        "# Define configurations based on window sizes and activation functions\n",
        "window_sizes = [0, 1]\n",
        "activation_functions = ['identity', 'tanh', 'relu', 'sigmoid']\n",
        "\n",
        "for w in window_sizes:\n",
        "    for activation in activation_functions:\n",
        "        configs.append({\n",
        "            'window_size': w,\n",
        "            'activation_function': activation\n",
        "        })\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\\nTraining with context window size w={config['window_size']} and {config['activation_function']} activation function\")\n",
        "\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "\n",
        "    POST = POSTagger_embed(w=config['window_size'], vocab_size=len(embedding_df))\n",
        "    model = NN_activation(POST, pretrained_embeddings, twitter_to_index, activation_function=config['activation_function'])\n",
        "\n",
        "    model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "\n",
        "    result = model.fit()\n",
        "\n",
        "    if isinstance(result, tuple):\n",
        "        result = {\n",
        "            'train_loss': result[0],\n",
        "            'dev_loss': result[1],\n",
        "            'devtest_loss': result[2],\n",
        "            'dev_accuracy': result[3],\n",
        "            'devtest_accuracy': result[4]\n",
        "        }\n",
        "\n",
        "    result.update(config)\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "6mhr3ROAqzny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcf11c4-98bc-47af-a2c3-91257cbfb400"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0 and identity activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0195\n",
            "Loss: 0.6811, Accuracy: 81.9125%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5119\n",
            "Loss: 0.5864, Accuracy: 82.5347%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4376\n",
            "Loss: 0.6198, Accuracy: 82.9496%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4094\n",
            "Loss: 0.5471, Accuracy: 84.0282%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3883\n",
            "Loss: 0.5787, Accuracy: 83.8623%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3765\n",
            "Loss: 0.5977, Accuracy: 82.5347%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3701\n",
            "Loss: 0.5595, Accuracy: 84.0904%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3641\n",
            "Loss: 0.5660, Accuracy: 83.0948%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3588\n",
            "Loss: 0.5555, Accuracy: 82.5970%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5108, Accuracy: 83.3585%\n",
            "\n",
            "Training with context window size w=0 and tanh activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0248\n",
            "Loss: 0.6007, Accuracy: 82.9703%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5140\n",
            "Loss: 0.5560, Accuracy: 83.8208%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4437\n",
            "Loss: 0.5595, Accuracy: 83.6134%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4087\n",
            "Loss: 0.5344, Accuracy: 84.2564%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3867\n",
            "Loss: 0.5601, Accuracy: 83.3022%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3754\n",
            "Loss: 0.6091, Accuracy: 82.3895%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3656\n",
            "Loss: 0.5896, Accuracy: 83.0326%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3560\n",
            "Loss: 0.5604, Accuracy: 83.5511%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3536\n",
            "Loss: 0.5617, Accuracy: 83.9660%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5277, Accuracy: 83.9405%\n",
            "\n",
            "Training with context window size w=0 and relu activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0687\n",
            "Loss: 0.6175, Accuracy: 81.8502%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5034\n",
            "Loss: 0.5916, Accuracy: 82.3273%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4227\n",
            "Loss: 0.5544, Accuracy: 84.1319%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3850\n",
            "Loss: 0.5508, Accuracy: 83.8000%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3661\n",
            "Loss: 0.5416, Accuracy: 84.2564%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3525\n",
            "Loss: 0.5536, Accuracy: 84.2979%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3429\n",
            "Loss: 0.5604, Accuracy: 83.7378%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3361\n",
            "Loss: 0.5538, Accuracy: 84.0490%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3298\n",
            "Loss: 0.5603, Accuracy: 83.9245%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3251\n",
            "Loss: 0.5717, Accuracy: 84.0075%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5330, Accuracy: 84.0052%\n",
            "\n",
            "Training with context window size w=0 and sigmoid activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 2.3212\n",
            "Loss: 1.1416, Accuracy: 69.2802%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.8557\n",
            "Loss: 0.7634, Accuracy: 79.5478%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.5847\n",
            "Loss: 0.6550, Accuracy: 81.8917%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4739\n",
            "Loss: 0.6154, Accuracy: 83.1570%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.4201\n",
            "Loss: 0.6117, Accuracy: 82.6799%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3957\n",
            "Loss: 0.6232, Accuracy: 82.7629%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3813\n",
            "Loss: 0.6099, Accuracy: 83.3644%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3682\n",
            "Loss: 0.6137, Accuracy: 83.9452%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3610\n",
            "Loss: 0.6198, Accuracy: 83.3437%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3553\n",
            "Loss: 0.6125, Accuracy: 82.5555%\n",
            "------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3497\n",
            "Loss: 0.6140, Accuracy: 82.5140%\n",
            "------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3438\n",
            "Loss: 0.6032, Accuracy: 83.4267%\n",
            "------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.3394\n",
            "Loss: 0.5959, Accuracy: 83.0948%\n",
            "------------------------------------\n",
            "Epoch 13\n",
            "Epoch 13, Loss: 0.3375\n",
            "Loss: 0.6093, Accuracy: 83.3437%\n",
            "------------------------------------\n",
            "Epoch 14\n",
            "Epoch 14, Loss: 0.3328\n",
            "Loss: 0.6028, Accuracy: 83.1778%\n",
            "------------------------------------\n",
            "Epoch 15\n",
            "Epoch 15, Loss: 0.3295\n",
            "Loss: 0.6173, Accuracy: 83.6963%\n",
            "------------------------------------\n",
            "Epoch 16\n",
            "Epoch 16, Loss: 0.3275\n",
            "Loss: 0.6122, Accuracy: 83.6756%\n",
            "------------------------------------\n",
            "Epoch 17\n",
            "Epoch 17, Loss: 0.3261\n",
            "Loss: 0.6007, Accuracy: 82.9081%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5420, Accuracy: 83.9189%\n",
            "\n",
            "Training with context window size w=1 and identity activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9053\n",
            "Loss: 0.5271, Accuracy: 85.0031%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4025\n",
            "Loss: 0.4706, Accuracy: 86.5795%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3083\n",
            "Loss: 0.4727, Accuracy: 87.3055%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2430\n",
            "Loss: 0.5193, Accuracy: 87.1189%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2100\n",
            "Loss: 0.6068, Accuracy: 85.0446%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1769\n",
            "Loss: 0.5870, Accuracy: 86.0610%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1578\n",
            "Loss: 0.6052, Accuracy: 85.6669%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5556, Accuracy: 86.7428%\n",
            "\n",
            "Training with context window size w=1 and tanh activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9150\n",
            "Loss: 0.4976, Accuracy: 86.2062%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4050\n",
            "Loss: 0.4964, Accuracy: 86.3514%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3027\n",
            "Loss: 0.5008, Accuracy: 85.9365%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2439\n",
            "Loss: 0.4882, Accuracy: 87.4507%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2053\n",
            "Loss: 0.5113, Accuracy: 86.6625%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1750\n",
            "Loss: 0.5648, Accuracy: 86.6210%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1394\n",
            "Loss: 0.6150, Accuracy: 86.1647%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1285\n",
            "Loss: 0.6146, Accuracy: 86.1647%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.1114\n",
            "Loss: 0.6374, Accuracy: 85.7913%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5834, Accuracy: 86.7213%\n",
            "\n",
            "Training with context window size w=1 and relu activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9769\n",
            "Loss: 0.5185, Accuracy: 85.0653%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3989\n",
            "Loss: 0.4530, Accuracy: 87.2433%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2890\n",
            "Loss: 0.4741, Accuracy: 87.0359%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2227\n",
            "Loss: 0.5058, Accuracy: 86.8285%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1769\n",
            "Loss: 0.5533, Accuracy: 86.7247%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1443\n",
            "Loss: 0.5685, Accuracy: 86.7247%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1209\n",
            "Loss: 0.5942, Accuracy: 86.9114%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5441, Accuracy: 87.4326%\n",
            "\n",
            "Training with context window size w=1 and sigmoid activation function\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 2.0782\n",
            "Loss: 0.9918, Accuracy: 71.8108%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.7165\n",
            "Loss: 0.6295, Accuracy: 82.8666%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4685\n",
            "Loss: 0.5613, Accuracy: 84.3393%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.3604\n",
            "Loss: 0.5274, Accuracy: 85.6046%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2937\n",
            "Loss: 0.5065, Accuracy: 86.9737%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.2525\n",
            "Loss: 0.5253, Accuracy: 86.3306%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.2229\n",
            "Loss: 0.5068, Accuracy: 87.2018%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.2005\n",
            "Loss: 0.5325, Accuracy: 86.6625%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.1784\n",
            "Loss: 0.5401, Accuracy: 86.8492%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.1610\n",
            "Loss: 0.5540, Accuracy: 86.8492%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5211, Accuracy: 87.4326%\n",
            "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
            "0    0.358820  0.555467      0.510834     82.596972         83.358482   \n",
            "1    0.353601  0.561695      0.527724     83.965982         83.940504   \n",
            "2    0.325073  0.571744      0.533019     84.007467         84.005174   \n",
            "3    0.326105  0.600742      0.542025     82.908110         83.918948   \n",
            "4    0.157767  0.605181      0.555561     85.666874         86.742833   \n",
            "5    0.111449  0.637382      0.583445     85.791330         86.721276   \n",
            "6    0.120856  0.594233      0.544088     86.911429         87.432636   \n",
            "7    0.160995  0.554030      0.521123     86.849201         87.432636   \n",
            "\n",
            "   window_size activation_function  \n",
            "0            0            identity  \n",
            "1            0                tanh  \n",
            "2            0                relu  \n",
            "3            0             sigmoid  \n",
            "4            1            identity  \n",
            "5            1                tanh  \n",
            "6            1                relu  \n",
            "7            1             sigmoid  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overly, ReLU or Sigmoid produce the highest test accuracy, but it doesn't differ that much."
      ],
      "metadata": {
        "id": "MsBgC90OLgyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "4diGXfditMFQ",
        "outputId": "d41995f4-bf0b-4a1a-d160-1160c38ee6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.358820  0.555467      0.510834     82.596972         83.358482   \n",
              "1    0.353601  0.561695      0.527724     83.965982         83.940504   \n",
              "2    0.325073  0.571744      0.533019     84.007467         84.005174   \n",
              "3    0.326105  0.600742      0.542025     82.908110         83.918948   \n",
              "4    0.157767  0.605181      0.555561     85.666874         86.742833   \n",
              "5    0.111449  0.637382      0.583445     85.791330         86.721276   \n",
              "6    0.120856  0.594233      0.544088     86.911429         87.432636   \n",
              "7    0.160995  0.554030      0.521123     86.849201         87.432636   \n",
              "\n",
              "   window_size activation_function  \n",
              "0            0            identity  \n",
              "1            0                tanh  \n",
              "2            0                relu  \n",
              "3            0             sigmoid  \n",
              "4            1            identity  \n",
              "5            1                tanh  \n",
              "6            1                relu  \n",
              "7            1             sigmoid  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9f3beb9-6fdf-4585-835a-18d1e8c69416\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "      <th>activation_function</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.358820</td>\n",
              "      <td>0.555467</td>\n",
              "      <td>0.510834</td>\n",
              "      <td>82.596972</td>\n",
              "      <td>83.358482</td>\n",
              "      <td>0</td>\n",
              "      <td>identity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.353601</td>\n",
              "      <td>0.561695</td>\n",
              "      <td>0.527724</td>\n",
              "      <td>83.965982</td>\n",
              "      <td>83.940504</td>\n",
              "      <td>0</td>\n",
              "      <td>tanh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.325073</td>\n",
              "      <td>0.571744</td>\n",
              "      <td>0.533019</td>\n",
              "      <td>84.007467</td>\n",
              "      <td>84.005174</td>\n",
              "      <td>0</td>\n",
              "      <td>relu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.326105</td>\n",
              "      <td>0.600742</td>\n",
              "      <td>0.542025</td>\n",
              "      <td>82.908110</td>\n",
              "      <td>83.918948</td>\n",
              "      <td>0</td>\n",
              "      <td>sigmoid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.157767</td>\n",
              "      <td>0.605181</td>\n",
              "      <td>0.555561</td>\n",
              "      <td>85.666874</td>\n",
              "      <td>86.742833</td>\n",
              "      <td>1</td>\n",
              "      <td>identity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.111449</td>\n",
              "      <td>0.637382</td>\n",
              "      <td>0.583445</td>\n",
              "      <td>85.791330</td>\n",
              "      <td>86.721276</td>\n",
              "      <td>1</td>\n",
              "      <td>tanh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.120856</td>\n",
              "      <td>0.594233</td>\n",
              "      <td>0.544088</td>\n",
              "      <td>86.911429</td>\n",
              "      <td>87.432636</td>\n",
              "      <td>1</td>\n",
              "      <td>relu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.160995</td>\n",
              "      <td>0.554030</td>\n",
              "      <td>0.521123</td>\n",
              "      <td>86.849201</td>\n",
              "      <td>87.432636</td>\n",
              "      <td>1</td>\n",
              "      <td>sigmoid</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9f3beb9-6fdf-4585-835a-18d1e8c69416')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d9f3beb9-6fdf-4585-835a-18d1e8c69416 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d9f3beb9-6fdf-4585-835a-18d1e8c69416');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e5ea951a-2c31-41e7-aaef-5dc4b00d6069\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5ea951a-2c31-41e7-aaef-5dc4b00d6069')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e5ea951a-2c31-41e7-aaef-5dc4b00d6069 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Experiment with w = 2 and compare the results to w = 0 and 1"
      ],
      "metadata": {
        "id": "Ty3sB4vNllvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [{'window_size': w} for w in [0, 1, 2]]\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\\nTraining with context window size w={config['window_size']}\")\n",
        "    pretrained_embeddings = torch.tensor(embedding_df.values)\n",
        "    POST = POSTagger_embed(w=config['window_size'], vocab_size=len(embedding_df))\n",
        "    model = NN_embed(POST, pretrained_embeddings, twitter_to_index)\n",
        "    model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "    result = model.fit()\n",
        "\n",
        "    if isinstance(result, tuple):\n",
        "        result = {\n",
        "            'train_loss': result[0],\n",
        "            'dev_loss': result[1],\n",
        "            'devtest_loss': result[2],\n",
        "            'dev_accuracy': result[3],\n",
        "            'devtest_accuracy': result[4]\n",
        "        }\n",
        "\n",
        "    result.update(config)\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NxuldLnflU2W",
        "outputId": "ef266cb7-c1b5-416f-c8a2-9ef424b705c4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with context window size w=0\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.0315\n",
            "Loss: 0.5859, Accuracy: 83.3644%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5158\n",
            "Loss: 0.6137, Accuracy: 82.5347%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4422\n",
            "Loss: 0.6108, Accuracy: 83.3644%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4095\n",
            "Loss: 0.5785, Accuracy: 83.4267%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.3921\n",
            "Loss: 0.6036, Accuracy: 81.4146%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.3782\n",
            "Loss: 0.5565, Accuracy: 83.8415%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.3681\n",
            "Loss: 0.5460, Accuracy: 83.8415%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.3606\n",
            "Loss: 0.5430, Accuracy: 84.3808%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.3580\n",
            "Loss: 0.5547, Accuracy: 83.3022%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.3491\n",
            "Loss: 0.6111, Accuracy: 82.1821%\n",
            "------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.3466\n",
            "Loss: 0.5705, Accuracy: 83.3852%\n",
            "------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.3429\n",
            "Loss: 0.5649, Accuracy: 83.8208%\n",
            "------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.3397\n",
            "Loss: 0.5673, Accuracy: 83.8000%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5138, Accuracy: 84.2854%\n",
            "\n",
            "Training with context window size w=1\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9199\n",
            "Loss: 0.5322, Accuracy: 84.7542%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4021\n",
            "Loss: 0.4678, Accuracy: 87.0359%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3016\n",
            "Loss: 0.4828, Accuracy: 86.4551%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2467\n",
            "Loss: 0.5219, Accuracy: 86.6625%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2027\n",
            "Loss: 0.5392, Accuracy: 86.4758%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1695\n",
            "Loss: 0.5797, Accuracy: 86.5173%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1488\n",
            "Loss: 0.5850, Accuracy: 86.6210%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.5338, Accuracy: 87.4326%\n",
            "\n",
            "Training with context window size w=2\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9153\n",
            "Loss: 0.5298, Accuracy: 85.0239%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.3978\n",
            "Loss: 0.4711, Accuracy: 86.8285%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2827\n",
            "Loss: 0.5155, Accuracy: 86.3099%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2139\n",
            "Loss: 0.6027, Accuracy: 86.1854%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1590\n",
            "Loss: 0.5943, Accuracy: 86.0402%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1173\n",
            "Loss: 0.6281, Accuracy: 85.7084%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.0865\n",
            "Loss: 0.6565, Accuracy: 85.9988%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.6241, Accuracy: 87.1093%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.339709  0.567317      0.513823     83.800041         84.285406   \n",
              "1    0.148761  0.584993      0.533822     86.621033         87.432636   \n",
              "2    0.086527  0.656453      0.624116     85.998755         87.109291   \n",
              "\n",
              "   window_size  \n",
              "0            0  \n",
              "1            1  \n",
              "2            2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58c92fde-69da-47b0-aac1-c914df5b07e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.339709</td>\n",
              "      <td>0.567317</td>\n",
              "      <td>0.513823</td>\n",
              "      <td>83.800041</td>\n",
              "      <td>84.285406</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.148761</td>\n",
              "      <td>0.584993</td>\n",
              "      <td>0.533822</td>\n",
              "      <td>86.621033</td>\n",
              "      <td>87.432636</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.086527</td>\n",
              "      <td>0.656453</td>\n",
              "      <td>0.624116</td>\n",
              "      <td>85.998755</td>\n",
              "      <td>87.109291</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58c92fde-69da-47b0-aac1-c914df5b07e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-58c92fde-69da-47b0-aac1-c914df5b07e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-58c92fde-69da-47b0-aac1-c914df5b07e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-225a81fa-4d97-4a44-9cf8-5f7f30be24f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-225a81fa-4d97-4a44-9cf8-5f7f30be24f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-225a81fa-4d97-4a44-9cf8-5f7f30be24f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window size 1 produces the best test accuracy, and we can also see that the bigger window size does not make the test accuracy higher.\n"
      ],
      "metadata": {
        "id": "HfO9_ImrL5Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "vf04w9kSuA1m",
        "outputId": "d523160c-5707-4c12-e51f-af69ba2426ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_loss  dev_loss  devtest_loss  dev_accuracy  devtest_accuracy  \\\n",
              "0    0.339709  0.567317      0.513823     83.800041         84.285406   \n",
              "1    0.148761  0.584993      0.533822     86.621033         87.432636   \n",
              "2    0.086527  0.656453      0.624116     85.998755         87.109291   \n",
              "\n",
              "   window_size  \n",
              "0            0  \n",
              "1            1  \n",
              "2            2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b68a2403-17c7-4c56-baa5-5fe000f065c3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "      <th>window_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.339709</td>\n",
              "      <td>0.567317</td>\n",
              "      <td>0.513823</td>\n",
              "      <td>83.800041</td>\n",
              "      <td>84.285406</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.148761</td>\n",
              "      <td>0.584993</td>\n",
              "      <td>0.533822</td>\n",
              "      <td>86.621033</td>\n",
              "      <td>87.432636</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.086527</td>\n",
              "      <td>0.656453</td>\n",
              "      <td>0.624116</td>\n",
              "      <td>85.998755</td>\n",
              "      <td>87.109291</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b68a2403-17c7-4c56-baa5-5fe000f065c3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b68a2403-17c7-4c56-baa5-5fe000f065c3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b68a2403-17c7-4c56-baa5-5fe000f065c3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-12afdead-1d6e-45b9-83e9-6ab5b652a1b3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12afdead-1d6e-45b9-83e9-6ab5b652a1b3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-12afdead-1d6e-45b9-83e9-6ab5b652a1b3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.5 RNN Taggers"
      ],
      "metadata": {
        "id": "H1UVKLqumYTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger_Additional:\n",
        "  def __init__(self, w, vocab_size):\n",
        "    self.w = w\n",
        "    self.dim_e = 50\n",
        "    self.dim_h = 512\n",
        "    self.dim_s = len(tag_to_ix)\n",
        "    self.vocab_size = len(twitter_to_index)\n",
        "    self.batch_size = 32\n",
        "    self.epochs = 20"
      ],
      "metadata": {
        "id": "0vtHtGNyTIVd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_Additional(nn.Module):\n",
        "    def __init__(self, POSTagger, pertrained_embeddings, twitter_to_index, rnn_type, bidirectional):\n",
        "        super().__init__()\n",
        "        self.POSTagger = POSTagger\n",
        "        self.embeddings = nn.Embedding(self.POSTagger.vocab_size, self.POSTagger.dim_e)\n",
        "        self.hidden = nn.Linear((2 * self.POSTagger.w + 1) * self.POSTagger.dim_e, self.POSTagger.dim_h)\n",
        "        direction_factor = 2 if bidirectional else 1\n",
        "        self.output = nn.Linear(self.POSTagger.dim_h * direction_factor, self.POSTagger.dim_s)\n",
        "        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n",
        "        self.rnn = rnn_class(self.POSTagger.dim_e * (2 * self.POSTagger.w + 1), self.POSTagger.dim_h, batch_first=True, bidirectional=bidirectional)\n",
        "        # Output layer\n",
        "\n",
        "        self.dropout_first = nn.Dropout(0.5)\n",
        "\n",
        "        self.loss_function = F.cross_entropy\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=0.02)\n",
        "\n",
        "        # Initialize embeddings with pretrained vectors\n",
        "        self.init_embeddings(pretrained_embeddings, twitter_to_index)\n",
        "\n",
        "    def init_embeddings(self, pretrained_embeddings, twitter_to_index):\n",
        "        \"\"\"\n",
        "        Initialize the embedding layer with pretrained embeddings.\n",
        "        Words not found in the pretrained list will be initialized with the UUUNKKK vector.\n",
        "        \"\"\"\n",
        "        initrange = 0.01\n",
        "        self.embeddings.weight.data = pretrained_embeddings\n",
        "     #   self.rnn.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1, (2 * self.POSTagger.w + 1) * self.POSTagger.dim_e)\n",
        "        embeds = self.dropout_first(embeds)\n",
        "        hidden_out = self.rnn(embeds)\n",
        "        hidden_out, _ = self.rnn(embeds)\n",
        "        hidden_activated = F.relu(hidden_out)\n",
        "        tag_space = self.output(hidden_activated)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
        "        return tag_scores\n",
        "\n",
        "    def load_data(self, train_sentence, dev_sentence, devtest_sentence, twitter_to_index, tag_to_ix):\n",
        "        self.train_data, self.train_label = transfer_sentence(train_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.dev_data, self.dev_label = transfer_sentence(dev_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "        self.devtest_data, self.devtest_label = transfer_sentence(devtest_sentence, twitter_to_index, tag_to_ix, self.POSTagger.w)\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        log_prob = self.forward(x)\n",
        "        loss = self.loss_function(log_prob, y, reduction='sum')\n",
        "        return loss\n",
        "\n",
        "    def run_grad(self, x, y):\n",
        "        loss = self.get_loss(x, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def one_epoch(self, epoch, sentence, label):\n",
        "        n = sentence.shape[0]\n",
        "        idx = np.arange(0, n)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        sentence_s = sentence[idx]\n",
        "        label_s = label[idx]\n",
        "\n",
        "        train_loss = 0\n",
        "        for i in range(0, n, self.POSTagger.batch_size):\n",
        "            x = torch.tensor(sentence_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            y = torch.tensor(label_s[i:i + self.POSTagger.batch_size], dtype=torch.long)\n",
        "            loss = self.run_grad(x, y)\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= n\n",
        "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}')\n",
        "        return train_loss\n",
        "\n",
        "    def test(self, sentence, label):\n",
        "        self.eval()\n",
        "        n = sentence.shape[0]\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            x = torch.tensor(sentence, dtype=torch.long)\n",
        "            y = torch.tensor(label, dtype=torch.long)\n",
        "            loss = self.get_loss(x, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            log_probs = self.forward(x)\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "            correct += (y == predicted).sum().item()\n",
        "\n",
        "            test_loss /= n\n",
        "            accuracy = (correct / n) * 100\n",
        "            print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}%\")\n",
        "            return test_loss, accuracy\n",
        "\n",
        "    def fit(self):\n",
        "        best_dev_loss = np.inf\n",
        "        epochs_without_improvement = 0\n",
        "        patience = 5\n",
        "\n",
        "        for i in range(self.POSTagger.epochs):\n",
        "            print('Epoch', i)\n",
        "            train_loss = self.one_epoch(i, self.train_data, self.train_label)\n",
        "            dev_loss, dev_accuracy = self.test(self.dev_data, self.dev_label)\n",
        "\n",
        "            if dev_loss < best_dev_loss:\n",
        "                best_dev_loss = dev_loss\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(\"Early stopping due to no improvement.\")\n",
        "                    break\n",
        "            print('------------------------------------')\n",
        "        print('\\nTesting Result')\n",
        "        devtest_loss, devtest_accuracy = self.test(self.devtest_data, self.devtest_label)\n",
        "        return train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy"
      ],
      "metadata": {
        "id": "iNtwnZFQlnny"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POST = POSTagger_Additional(w=2, vocab_size = len(embedding_df))\n",
        "def train_and_evaluate_models(configurations):\n",
        "    results = []\n",
        "\n",
        "    for config in configurations:\n",
        "        rnn_type, bidirectional = config['rnn_type'], config['bidirectional']\n",
        "        print(f\"\\nTraining model with RNN type: {rnn_type} | Bidirectional: {str(bidirectional)}\")\n",
        "\n",
        "        model = NN_Additional(POST, pretrained_embeddings, twitter_to_index, rnn_type=rnn_type, bidirectional=bidirectional)\n",
        "\n",
        "        model.load_data(train_sentences, dev_sentences, devtest_sentences, twitter_to_index, tag_to_ix)\n",
        "\n",
        "        train_loss, dev_loss, devtest_loss, dev_accuracy, devtest_accuracy = model.fit()\n",
        "\n",
        "        results.append({\n",
        "            'rnn_type': rnn_type,\n",
        "            'bidirectional': bidirectional,\n",
        "            'train_loss': train_loss,\n",
        "            'dev_loss': dev_loss,\n",
        "            'devtest_loss': devtest_loss,\n",
        "            'dev_accuracy': dev_accuracy,\n",
        "            'devtest_accuracy': devtest_accuracy\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "configurations = [\n",
        "    {'rnn_type': 'LSTM', 'bidirectional': True},\n",
        "    {'rnn_type': 'RNN', 'bidirectional': True},\n",
        "    {'rnn_type': 'GRU', 'bidirectional': True},\n",
        "    {'rnn_type': 'LSTM', 'bidirectional': False},\n",
        "    {'rnn_type': 'RNN', 'bidirectional': False},\n",
        "    {'rnn_type': 'GRU', 'bidirectional': False}\n",
        "]\n",
        "\n",
        "results_df = train_and_evaluate_models(configurations)\n",
        "\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iv_l64ACJ_n1",
        "outputId": "51d1c3ba-a049-4a65-b672-aa70f9673305"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with RNN type: LSTM | Bidirectional: True\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.2639\n",
            "Loss: 0.5565, Accuracy: 83.8830%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.2705\n",
            "Loss: 0.5150, Accuracy: 86.5173%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.1618\n",
            "Loss: 0.5580, Accuracy: 86.4136%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.1147\n",
            "Loss: 0.5770, Accuracy: 86.4758%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.0847\n",
            "Loss: 0.6352, Accuracy: 86.0610%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.0655\n",
            "Loss: 0.6614, Accuracy: 85.9365%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.0553\n",
            "Loss: 0.7078, Accuracy: 85.7084%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.6527, Accuracy: 86.3333%\n",
            "\n",
            "Training model with RNN type: RNN | Bidirectional: True\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 7.7915\n",
            "Loss: 6.1588, Accuracy: 28.7077%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 4.1154\n",
            "Loss: 1.9345, Accuracy: 52.3128%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 2.2627\n",
            "Loss: 1.7030, Accuracy: 58.0792%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 1.8843\n",
            "Loss: 1.3996, Accuracy: 61.8959%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 1.6687\n",
            "Loss: 1.3214, Accuracy: 64.6961%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 1.5844\n",
            "Loss: 1.2956, Accuracy: 68.4713%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 1.5184\n",
            "Loss: 1.1351, Accuracy: 69.5499%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 1.5110\n",
            "Loss: 1.3594, Accuracy: 65.3806%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 1.6578\n",
            "Loss: 1.3351, Accuracy: 71.5619%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 1.7286\n",
            "Loss: 1.4985, Accuracy: 68.2638%\n",
            "------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 1.8999\n",
            "Loss: 3.5631, Accuracy: 71.6449%\n",
            "------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 2.3826\n",
            "Loss: 1.8335, Accuracy: 71.8316%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 1.7969, Accuracy: 72.0845%\n",
            "\n",
            "Training model with RNN type: GRU | Bidirectional: True\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9942\n",
            "Loss: 0.7784, Accuracy: 76.4364%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.4648\n",
            "Loss: 0.7121, Accuracy: 80.6264%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.3397\n",
            "Loss: 0.7117, Accuracy: 82.1406%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.2659\n",
            "Loss: 0.6922, Accuracy: 83.3230%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.2260\n",
            "Loss: 0.6973, Accuracy: 83.2607%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.1748\n",
            "Loss: 0.7352, Accuracy: 83.0741%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.1444\n",
            "Loss: 0.7772, Accuracy: 83.6134%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.1139\n",
            "Loss: 0.7547, Accuracy: 84.5675%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.0841\n",
            "Loss: 0.7921, Accuracy: 84.2771%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.7369, Accuracy: 85.4063%\n",
            "\n",
            "Training model with RNN type: LSTM | Bidirectional: False\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.8898\n",
            "Loss: 0.6303, Accuracy: 82.0369%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.2996\n",
            "Loss: 0.5815, Accuracy: 84.1112%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.2115\n",
            "Loss: 0.6092, Accuracy: 84.6297%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.1631\n",
            "Loss: 0.6166, Accuracy: 84.7957%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 0.1242\n",
            "Loss: 0.6723, Accuracy: 84.2979%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.0996\n",
            "Loss: 0.6662, Accuracy: 84.9409%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.0757\n",
            "Loss: 0.7282, Accuracy: 84.2356%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 0.6445, Accuracy: 85.7512%\n",
            "\n",
            "Training model with RNN type: RNN | Bidirectional: False\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 0.9946\n",
            "Loss: 0.8147, Accuracy: 76.9965%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 0.5318\n",
            "Loss: 0.9714, Accuracy: 78.2203%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 0.4980\n",
            "Loss: 1.1135, Accuracy: 77.2869%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 0.4928\n",
            "Loss: 1.1371, Accuracy: 77.4943%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 1.1358\n",
            "Loss: 3.2511, Accuracy: 64.4265%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 6.3655\n",
            "Loss: 8.0305, Accuracy: 30.0145%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 7.9521, Accuracy: 31.5585%\n",
            "\n",
            "Training model with RNN type: GRU | Bidirectional: False\n",
            "Epoch 0\n",
            "Epoch 0, Loss: 1.9687\n",
            "Loss: 1.6289, Accuracy: 53.9307%\n",
            "------------------------------------\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 1.3818\n",
            "Loss: 1.5566, Accuracy: 56.8139%\n",
            "------------------------------------\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 1.2308\n",
            "Loss: 1.4748, Accuracy: 59.5312%\n",
            "------------------------------------\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 1.1502\n",
            "Loss: 1.2920, Accuracy: 63.3893%\n",
            "------------------------------------\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 1.0813\n",
            "Loss: 1.2396, Accuracy: 67.1852%\n",
            "------------------------------------\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 0.9804\n",
            "Loss: 1.2021, Accuracy: 67.4756%\n",
            "------------------------------------\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 0.9257\n",
            "Loss: 1.1292, Accuracy: 71.4997%\n",
            "------------------------------------\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 0.8498\n",
            "Loss: 1.3128, Accuracy: 69.0106%\n",
            "------------------------------------\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 0.7897\n",
            "Loss: 1.3081, Accuracy: 70.2966%\n",
            "------------------------------------\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 0.7369\n",
            "Loss: 1.0987, Accuracy: 73.5739%\n",
            "------------------------------------\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 0.7033\n",
            "Loss: 1.1244, Accuracy: 73.5739%\n",
            "------------------------------------\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 0.6518\n",
            "Loss: 1.1481, Accuracy: 73.9473%\n",
            "------------------------------------\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 0.6043\n",
            "Loss: 1.2042, Accuracy: 73.0139%\n",
            "------------------------------------\n",
            "Epoch 13\n",
            "Epoch 13, Loss: 0.5841\n",
            "Loss: 1.0799, Accuracy: 75.5238%\n",
            "------------------------------------\n",
            "Epoch 14\n",
            "Epoch 14, Loss: 0.5327\n",
            "Loss: 1.0359, Accuracy: 76.7268%\n",
            "------------------------------------\n",
            "Epoch 15\n",
            "Epoch 15, Loss: 0.4894\n",
            "Loss: 1.0826, Accuracy: 76.1875%\n",
            "------------------------------------\n",
            "Epoch 16\n",
            "Epoch 16, Loss: 0.4637\n",
            "Loss: 1.1331, Accuracy: 75.7727%\n",
            "------------------------------------\n",
            "Epoch 17\n",
            "Epoch 17, Loss: 0.4344\n",
            "Loss: 1.0594, Accuracy: 77.8262%\n",
            "------------------------------------\n",
            "Epoch 18\n",
            "Epoch 18, Loss: 0.4111\n",
            "Loss: 1.0627, Accuracy: 76.3742%\n",
            "------------------------------------\n",
            "Epoch 19\n",
            "Epoch 19, Loss: 0.3868\n",
            "Loss: 1.0757, Accuracy: 77.2661%\n",
            "Early stopping due to no improvement.\n",
            "\n",
            "Testing Result\n",
            "Loss: 1.0052, Accuracy: 78.1203%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  rnn_type  bidirectional  train_loss  dev_loss  devtest_loss  dev_accuracy  \\\n",
              "0     LSTM           True    0.055341  0.707769      0.652717     85.708359   \n",
              "1      RNN           True    2.382645  1.833507      1.796937     71.831570   \n",
              "2      GRU           True    0.084065  0.792066      0.736939     84.277121   \n",
              "3     LSTM          False    0.075654  0.728211      0.644470     84.235636   \n",
              "4      RNN          False    6.365451  8.030508      7.952078     30.014520   \n",
              "5      GRU          False    0.386780  1.075713      1.005192     77.266127   \n",
              "\n",
              "   devtest_accuracy  \n",
              "0         86.333261  \n",
              "1         72.084501  \n",
              "2         85.406338  \n",
              "3         85.751239  \n",
              "4         31.558526  \n",
              "5         78.120285  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b8d926b-be07-40a1-bf95-cde171b00a5e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rnn_type</th>\n",
              "      <th>bidirectional</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>dev_loss</th>\n",
              "      <th>devtest_loss</th>\n",
              "      <th>dev_accuracy</th>\n",
              "      <th>devtest_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>True</td>\n",
              "      <td>0.055341</td>\n",
              "      <td>0.707769</td>\n",
              "      <td>0.652717</td>\n",
              "      <td>85.708359</td>\n",
              "      <td>86.333261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RNN</td>\n",
              "      <td>True</td>\n",
              "      <td>2.382645</td>\n",
              "      <td>1.833507</td>\n",
              "      <td>1.796937</td>\n",
              "      <td>71.831570</td>\n",
              "      <td>72.084501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GRU</td>\n",
              "      <td>True</td>\n",
              "      <td>0.084065</td>\n",
              "      <td>0.792066</td>\n",
              "      <td>0.736939</td>\n",
              "      <td>84.277121</td>\n",
              "      <td>85.406338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>False</td>\n",
              "      <td>0.075654</td>\n",
              "      <td>0.728211</td>\n",
              "      <td>0.644470</td>\n",
              "      <td>84.235636</td>\n",
              "      <td>85.751239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNN</td>\n",
              "      <td>False</td>\n",
              "      <td>6.365451</td>\n",
              "      <td>8.030508</td>\n",
              "      <td>7.952078</td>\n",
              "      <td>30.014520</td>\n",
              "      <td>31.558526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>GRU</td>\n",
              "      <td>False</td>\n",
              "      <td>0.386780</td>\n",
              "      <td>1.075713</td>\n",
              "      <td>1.005192</td>\n",
              "      <td>77.266127</td>\n",
              "      <td>78.120285</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b8d926b-be07-40a1-bf95-cde171b00a5e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b8d926b-be07-40a1-bf95-cde171b00a5e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b8d926b-be07-40a1-bf95-cde171b00a5e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0ca382cd-94f0-4f3b-88c9-687c7643b869\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ca382cd-94f0-4f3b-88c9-687c7643b869')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0ca382cd-94f0-4f3b-88c9-687c7643b869 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}
